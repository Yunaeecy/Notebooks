{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction \n\nThis article will introduce more linear algebra concepts with the main focus on how these concepts are applied for dimensionality reduction, specially Principal Component Analysis (PCA).\n\n**When to use PCA?**\n\nHigh-dimensional data is a common issue experienced in machine learning practices, as we typically feed a large amount of features for model training. This results in the caveat of models having less interpretability and higher complexity — also known as the curse of dimensionality.\nPCA can be beneficial when the dataset is high-dimensional (i.e. contains many features) and it is widely applied for dimensionality reduction.\nIf you would like to practice these transformations in python and skip the manual calculations, we can use following code to perform these dot products and visualize the result of the transformation using `plt.quiver()` function.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n# define matrices and vector\nx_scaled_matrix = np.array([[2,0],[0,1]])\nmirror_matrix = np.array([[0,1],[1,0]])\nv = np.array([1,2])\n# matrix transformation\nmirrored_v = mirror_matrix.dot(v)\nx_scaled_v = x_scaled_matrix.dot(v)\n# plot transformed vectors\norigin = np.array([[0, 0], [0, 0]])\nplt.quiver(*origin, v[0], v[1], color=['black'],scale=10, label='original vector')\nplt.quiver(*origin, mirrored_v[0], mirrored_v[1] , color=['#D3E7EE'], scale=10, label='mirrored vector' )\nplt.quiver(*origin, x_scaled_v[0], x_scaled_v[1] , color=['#C6A477'], scale=10, label='x_scaled vector')\nplt.legend(loc =\"lower right\")","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:36:07.834707Z","iopub.execute_input":"2024-08-20T10:36:07.835126Z","iopub.status.idle":"2024-08-20T10:36:08.348188Z","shell.execute_reply.started":"2024-08-20T10:36:07.835064Z","shell.execute_reply":"2024-08-20T10:36:08.346912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Covariance matrix**\n\nCovariance matrix is another critical concept in PCA process that represents the data variance in the dataset. To understand the details of covariance matrix, we firstly need to know that covariance measures the magnitude of how one random variable varies with another random variable. For two random variable x and y, their covariance is formulated as below and higher covariance value indicates stronger correlation between two variables.\n\nWhen given a set of variables (e.g. x1, x2, … xn) in a dataset, covariance matrix is used for representing the covariance value between each variable pairs in a matrix format.\n\nMultiplying any vector with the covariance matrix will transform it towards the direction that captures the trend of variance in the original dataset.","metadata":{}},{"cell_type":"code","source":"# generate random variables x0 and x1\nimport random\nx0 = [round(random.uniform(-1, 1),2) for i in range(0,100)]\nx1 = [round(2 * i + random.uniform(-1, 1) ,2) for i in x0]\n\n# compute covariance matrix\nX = np.stack((x0, x1), axis=0)\ncovariance_matrix = np.cov(X)\nprint('covariance matrix\\n', covariance_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:36:08.352883Z","iopub.execute_input":"2024-08-20T10:36:08.353244Z","iopub.status.idle":"2024-08-20T10:36:08.362736Z","shell.execute_reply.started":"2024-08-20T10:36:08.353214Z","shell.execute_reply":"2024-08-20T10:36:08.361459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then transform some random vectors by taking the dot product between each of them and the covariance matrix.","metadata":{}},{"cell_type":"code","source":"# plot original data points\nplt.scatter(x0, x1, color=['#D3E7EE'])\n\n# vectors before transformation\nv_original = [np.array([[1,0.2]]), np.array([[-1,1.5]]), np.array([[1.5,-1.3]]), np.array([[1,1.4]])]\n\n# vectors after transformation\nfor v in v_original:\n    v_transformed = v.dot(covariance_matrix)\n    origin = np.array([[0, 0], [0, 0]])\n    plt.quiver(*origin, v[:, 0], v[:, 1], color=['black'], scale=4)\n    plt.quiver(*origin, v_transformed[:, 0], v_transformed[:, 1] , color=['#C6A477'], scale=10)\n\n# plot formatting\nplt.axis('scaled')   \nplt.xlim([-2.5,2.5])\nplt.ylim([-2.5,2.5])","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:36:08.370316Z","iopub.execute_input":"2024-08-20T10:36:08.371514Z","iopub.status.idle":"2024-08-20T10:36:08.788719Z","shell.execute_reply.started":"2024-08-20T10:36:08.371476Z","shell.execute_reply":"2024-08-20T10:36:08.787157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Covariance Matrix\n\nCovariance matrix is another critical concept in PCA process that represents the data variance in the dataset. To understand the details of covariance matrix, we firstly need to know that covariance measures the magnitude of how one random variable varies with another random variable.\n\nFor 2 radom variable x and y, their covariance is formulated as below and higher covariance value indicates stronger correlation between two variables.\n<div style=\"text-align: center\"><img src=\"https://pbs.twimg.com/media/GVawpDMbUAAxDpd?format=jpg&name=large\" width=\"100%\" heigh=\"100%\" alt=\"Retrieve&Re-Rank pipeline\"></div>\n\nWhen given a set of variables(e.g.x1,x2,...xn) in a dataset, covariance matrix is used for representing the covariance value between each variable pairs in a matrix format.\n\n<div style=\"text-align: center\"><img src=\"https://pbs.twimg.com/media/GVayLtqaMAYNfaJ?format=jpg&name=large\" width=\"100%\" heigh=\"100%\" alt=\"Retrieve&Re-Rank pipeline\"></div>\n\nMultiplying any vector with the covariance matrix will transform it towards the direction that captures the trend of variance in the original dataset.\n\nLet us use a simple example to simulate the effect of this transformation. Firstly, we randomly generate the variable x0, x1 and then compute the covariance matrix.","metadata":{}},{"cell_type":"code","source":"# generate random variables x0 and x1\nimport random\nx0 = [round(random.uniform(-1, 1),2) for i in range(0,100)]\nx1 = [round(2 * i + random.uniform(-1, 1) ,2) for i in x0]\n\n# compute covariance matrix\nX = np.stack((x0, x1), axis=0)\ncovariance_matrix = np.cov(X)\nprint('covariance matrix\\n', covariance_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:36:08.791407Z","iopub.execute_input":"2024-08-20T10:36:08.792529Z","iopub.status.idle":"2024-08-20T10:36:08.806086Z","shell.execute_reply.started":"2024-08-20T10:36:08.792473Z","shell.execute_reply":"2024-08-20T10:36:08.804720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we transform some random vectors by taking the dot product between each of them and the covariance matrix.","metadata":{}},{"cell_type":"code","source":"# plot original data points\nplt.scatter(x0, x1, color=['#D3E7EE'])\n\n# vectors before transformation\nv_original = [np.array([[1,0.2]]), np.array([[-1,1.5]]), np.array([[1.5,-1.3]]), np.array([[1,1.4]])]\n\n# vectors after transformation\nfor v in v_original:\n    v_transformed = v.dot(covariance_matrix)\n    origin = np.array([[0, 0], [0, 0]])\n    plt.quiver(*origin, v[:, 0], v[:, 1], color=['black'], scale=4)\n    plt.quiver(*origin, v_transformed[:, 0], v_transformed[:, 1] , color=['#C6A477'], scale=10)\n\n# plot formatting\nplt.axis('scaled')   \nplt.xlim([-2.5,2.5])\nplt.ylim([-2.5,2.5])","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:38:26.443998Z","iopub.execute_input":"2024-08-20T10:38:26.444996Z","iopub.status.idle":"2024-08-20T10:38:26.884651Z","shell.execute_reply.started":"2024-08-20T10:38:26.444962Z","shell.execute_reply":"2024-08-20T10:38:26.883004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Original vetors prior to the transformation are in black, and after transformation are in brown. As you can see, the original vectors that are pointing at different directions have become more conformed to the general trend displayed in the original dataset(the blue dots). \nBecause of this property, covariance matrix is important  to PCA in terms of describing the relationship between features.\n\n# Eigenvalue and Eigenvector\nIn short: Eigenvector (v) of a matrix (A) remains at the same direction after the matrix transformation, hence Av = λv where  v represents the corresponding eigenvalue. Representing data using eigenvector and eigenvalue reduces the dimensionality while maintaining the data variance as much possible.\n\nTo bring more intuitions to this concept, we can use a simple demonstration. For example, we have the matrix [[0,1],[1,0]], and one of the eigenvector for matrix is [1,1] and the corresponding eigenvalue is 1.\n\n<div style=\"text-align: center\"><img src=\"https://pbs.twimg.com/media/GWYA3KubQAICxID?format=jpg&name=900x900\" width=\"100%\" heigh=\"100%\" alt=\"Retrieve&Re-Rank pipeline\"></div>","metadata":{}},{"cell_type":"markdown","source":"# Credit:\n\n\nhttps://towardsdatascience.com/a-visual-learners-guide-to-explain-implement-and-interpret-principal-component-analysis-cc9b345b75be","metadata":{}}]}