{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yunasheng/pyspark-daily?scriptVersionId=160463479\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"a979ec05","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-01-26T05:16:47.842968Z","iopub.status.busy":"2024-01-26T05:16:47.842275Z","iopub.status.idle":"2024-01-26T05:17:41.906926Z","shell.execute_reply":"2024-01-26T05:17:41.90598Z"},"papermill":{"duration":54.113396,"end_time":"2024-01-26T05:17:41.909971","exception":false,"start_time":"2024-01-26T05:16:47.796575","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pyspark\r\n","  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n","\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n","Building wheels for collected packages: pyspark\r\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n","\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=2495587a1c51d397f55a8c0aebcfa633e30fdc72202d4054c08ece5323f0405e\r\n","  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\r\n","Successfully built pyspark\r\n","Installing collected packages: pyspark\r\n","Successfully installed pyspark-3.5.0\r\n"]}],"source":["!pip install pyspark"]},{"cell_type":"markdown","id":"66eb367d","metadata":{"papermill":{"duration":0.05103,"end_time":"2024-01-26T05:17:42.01368","exception":false,"start_time":"2024-01-26T05:17:41.96265","status":"completed"},"tags":[]},"source":["* **PySpark Daily**"]},{"cell_type":"markdown","id":"1d2d4922","metadata":{"papermill":{"duration":0.050945,"end_time":"2024-01-26T05:17:42.17295","exception":false,"start_time":"2024-01-26T05:17:42.122005","status":"completed"},"tags":[]},"source":["# Setting data types via custom schema"]},{"cell_type":"markdown","id":"78cc246c","metadata":{"papermill":{"duration":0.050628,"end_time":"2024-01-26T05:17:42.274411","exception":false,"start_time":"2024-01-26T05:17:42.223783","status":"completed"},"tags":[]},"source":["Today's post is about schemes. PySpark tends to mimic a lot of SQL database aspects. Its standard practice to define a table scheme for our dataframe when either creating a dataframe or reading files"]},{"cell_type":"code","execution_count":2,"id":"ae440041","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:17:42.378721Z","iopub.status.busy":"2024-01-26T05:17:42.37833Z","iopub.status.idle":"2024-01-26T05:17:57.674283Z","shell.execute_reply":"2024-01-26T05:17:57.673304Z"},"papermill":{"duration":15.351559,"end_time":"2024-01-26T05:17:57.67712","exception":false,"start_time":"2024-01-26T05:17:42.325561","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/01/26 05:17:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-----+---+--------+\n","| name|age|  salary|\n","+-----+---+--------+\n","|Alice| 28|100000.0|\n","|  bob| 35|120000.0|\n","+-----+---+--------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n","\n","# Create a Sprk session\n","spark = SparkSession.builder.getOrCreate()\n","spark.sparkContext.setLogLevel(\"ERROR\")\n","\n","# Define the schema using structType and StructField\n","schema = StructType([\n","    StructField(\"name\", StringType(), True),\n","    StructField(\"age\", IntegerType(), True),\n","    StructField(\"salary\", FloatType(), True)\n","])\n","# create a dataframe with the defined schema\n","data = [(\"Alice\", 28, 100000.0),\n","       (\"bob\",35, 120000.0)]\n","df = spark.createDataFrame(data, schema)\n","\n","# show the dataframe with the defined schema\n","df.show()\n","spark.stop()"]},{"cell_type":"markdown","id":"699df4b8","metadata":{"papermill":{"duration":0.053797,"end_time":"2024-01-26T05:17:57.784607","exception":false,"start_time":"2024-01-26T05:17:57.73081","status":"completed"},"tags":[]},"source":["# Creating table view for spark dataframe"]},{"cell_type":"markdown","id":"24d605be","metadata":{"papermill":{"duration":0.052528,"end_time":"2024-01-26T05:17:57.890604","exception":false,"start_time":"2024-01-26T05:17:57.838076","status":"completed"},"tags":[]},"source":["Using SQL requests via spark.sql, you can work with the data the same way you would when working with databases"]},{"cell_type":"code","execution_count":3,"id":"e497f59e","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:17:57.997384Z","iopub.status.busy":"2024-01-26T05:17:57.996736Z","iopub.status.idle":"2024-01-26T05:18:00.609807Z","shell.execute_reply":"2024-01-26T05:18:00.608711Z"},"papermill":{"duration":2.670517,"end_time":"2024-01-26T05:18:00.613256","exception":false,"start_time":"2024-01-26T05:17:57.942739","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+----------+-----+\n","|      date|value|\n","+----------+-----+\n","|2020-01-01|   10|\n","|2020-01-02|   20|\n","|2020-01-03|   30|\n","|2020-01-04|   40|\n","|2020-01-05|   50|\n","+----------+-----+\n","\n"]}],"source":["# create a spark session\n","spark = SparkSession.builder.getOrCreate()\n","spark.sparkContext.setLogLevel(\"ERROR\")\n","\n","#Create a Pyspark DataFrame from a list of tuples\n","data = [\n","    (\"2020-01-01\",10),\n","    (\"2020-01-02\",20),\n","    (\"2020-01-03\",30),\n","    (\"2020-01-04\",40),\n","    (\"2020-01-05\",50)\n","]\n","df = spark.createDataFrame(data,[\"date\",\"value\"])\n","\n","# register the dataframe as a temporary table\n","df.createOrReplaceTempView(\"data_table\")\n","\n","# let preview our table\n","spark.sql('select * from data_table').show()"]},{"cell_type":"code","execution_count":4,"id":"85058032","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:00.726463Z","iopub.status.busy":"2024-01-26T05:18:00.726045Z","iopub.status.idle":"2024-01-26T05:18:02.703257Z","shell.execute_reply":"2024-01-26T05:18:02.702019Z"},"papermill":{"duration":2.036854,"end_time":"2024-01-26T05:18:02.706512","exception":false,"start_time":"2024-01-26T05:18:00.669658","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 2:============================================>              (3 + 1) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["+----------+-----+------------+\n","|      date|value|rolling_mean|\n","+----------+-----+------------+\n","|2020-01-01|   10|        15.0|\n","|2020-01-02|   20|        20.0|\n","|2020-01-03|   30|        30.0|\n","|2020-01-04|   40|        40.0|\n","|2020-01-05|   50|        45.0|\n","+----------+-----+------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# perform the rolling mean calculation using SQL notation\n","request = \"\"\"\n","SELECT date,\n","       value,\n","       AVG(value) OVER (ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) AS rolling_mean\n","FROM data_table\n","ORDER BY date\n","\"\"\"\n","\n","result = spark.sql(request)\n","result.show()"]},{"cell_type":"markdown","id":"3d02bc0f","metadata":{"papermill":{"duration":0.052811,"end_time":"2024-01-26T05:18:02.817827","exception":false,"start_time":"2024-01-26T05:18:02.765016","status":"completed"},"tags":[]},"source":["* If you wanted to replicate the same request using pyspark functions, you'd need to know what functionality to import\n","* For this problem we need to import from sql.functions and sql.window, so SQL notation is definitely convenient\n","* This is a big positive for pyspark, because you can do data analysis using big data without needing to know the library components"]},{"cell_type":"code","execution_count":5,"id":"e1515289","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:02.925766Z","iopub.status.busy":"2024-01-26T05:18:02.925367Z","iopub.status.idle":"2024-01-26T05:18:05.016926Z","shell.execute_reply":"2024-01-26T05:18:05.016014Z"},"papermill":{"duration":2.148259,"end_time":"2024-01-26T05:18:05.019654","exception":false,"start_time":"2024-01-26T05:18:02.871395","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+-----+------------+\n","|      date|value|rolling_mean|\n","+----------+-----+------------+\n","|2020-01-01|   10|        15.0|\n","|2020-01-02|   20|        20.0|\n","|2020-01-03|   30|        30.0|\n","|2020-01-04|   40|        40.0|\n","|2020-01-05|   50|        45.0|\n","+----------+-----+------------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import avg\n","from pyspark.sql.window import Window\n","\n","spark = SparkSession.builder.getOrCreate()\n","spark.sparkContext.setLogLevel(\"ERROR\")\n","\n","# Assuming you have a DataFrame called 'df' with columns 'value' and 'timestamp'\n","windowSpec = Window.orderBy(\"date\").rowsBetween(-1,1)\n","rollingMean = avg(df[\"value\"]).over(windowSpec)\n","\n","result = df.select(df[\"date\"], df[\"value\"], rollingMean.alias(\"rolling_mean\"))\n","result.show()\n","spark.stop()"]},{"cell_type":"markdown","id":"934146c8","metadata":{"papermill":{"duration":0.052027,"end_time":"2024-01-26T05:18:05.125072","exception":false,"start_time":"2024-01-26T05:18:05.073045","status":"completed"},"tags":[]},"source":["# Reading simple CSV files"]},{"cell_type":"markdown","id":"2b4f95da","metadata":{"papermill":{"duration":0.053174,"end_time":"2024-01-26T05:18:05.231516","exception":false,"start_time":"2024-01-26T05:18:05.178342","status":"completed"},"tags":[]},"source":["As mentioned before, Pyspark assigns StringType to each column when reading csv file"]},{"cell_type":"code","execution_count":6,"id":"4af04bc1","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:05.340237Z","iopub.status.busy":"2024-01-26T05:18:05.339793Z","iopub.status.idle":"2024-01-26T05:18:06.528331Z","shell.execute_reply":"2024-01-26T05:18:06.527265Z"},"papermill":{"duration":1.246138,"end_time":"2024-01-26T05:18:06.531549","exception":false,"start_time":"2024-01-26T05:18:05.285411","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["spark = SparkSession.builder.getOrCreate()\n","spark.read.csv('/kaggle/input/cognizant-artificial-intelligence/sample_sales_data (1).csv')"]},{"cell_type":"markdown","id":"dd752c8c","metadata":{"papermill":{"duration":0.073944,"end_time":"2024-01-26T05:18:06.681685","exception":false,"start_time":"2024-01-26T05:18:06.607741","status":"completed"},"tags":[]},"source":["* Some useful things to note when reading csv files:\n","* If your data contains a header; set header=True\n","* If you want to automatically determinecolumn types and set them; set inferSchema=True\n","* To add an option to .csv, add it before .csv by using .option, we can set differen settings for reading csv files here\n","* Set the delimiter(eg. via .option('delimiter', ';') if your data is separated by ';'"]},{"cell_type":"code","execution_count":7,"id":"bc81f27d","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:06.834479Z","iopub.status.busy":"2024-01-26T05:18:06.833358Z","iopub.status.idle":"2024-01-26T05:18:09.392637Z","shell.execute_reply":"2024-01-26T05:18:09.391498Z"},"papermill":{"duration":2.639691,"end_time":"2024-01-26T05:18:09.39645","exception":false,"start_time":"2024-01-26T05:18:06.756759","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n","|_c0|      transaction_id|          timestamp|          product_id|category|customer_type|unit_price|quantity|total|payment_type|\n","+---+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n","|  0|a1c82654-c52c-45b...|2022-03-02 09:51:38|3bc6c1ea-0198-46d...|   fruit|         gold|      3.99|       2| 7.98|    e-wallet|\n","+---+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n","only showing top 1 row\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+---+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n","|_c0|      transaction_id|          timestamp|          product_id|category|customer_type|unit_price|quantity|total|payment_type|\n","+---+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n","|  0|a1c82654-c52c-45b...|2022-03-02 09:51:38|3bc6c1ea-0198-46d...|   fruit|         gold|      3.99|       2| 7.98|    e-wallet|\n","|  1|931ad550-09e8-4da...|2022-03-06 10:33:59|ad81b46c-bf38-41c...|   fruit|     standard|      3.99|       1| 3.99|    e-wallet|\n","|  2|ae133534-6f61-4cd...|2022-03-04 17:20:21|7c55cbd4-f306-4c0...|   fruit|      premium|      0.19|       2| 0.38|    e-wallet|\n","|  3|157cebd9-aaf0-475...|2022-03-02 17:23:58|80da8348-1707-403...|   fruit|         gold|      0.19|       4| 0.76|    e-wallet|\n","|  4|a81a6cd3-5e0c-44a...|2022-03-05 14:32:43|7f5e86e6-f06f-45f...|   fruit|        basic|      4.49|       2| 8.98|  debit card|\n","+---+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n","only showing top 5 rows\n","\n","+---+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n","|_c0|      transaction_id|          timestamp|          product_id|category|customer_type|unit_price|quantity|total|payment_type|\n","+---+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n","|  0|a1c82654-c52c-45b...|2022-03-02 09:51:38|3bc6c1ea-0198-46d...|   fruit|         gold|      3.99|       2| 7.98|    e-wallet|\n","|  1|931ad550-09e8-4da...|2022-03-06 10:33:59|ad81b46c-bf38-41c...|   fruit|     standard|      3.99|       1| 3.99|    e-wallet|\n","|  2|ae133534-6f61-4cd...|2022-03-04 17:20:21|7c55cbd4-f306-4c0...|   fruit|      premium|      0.19|       2| 0.38|    e-wallet|\n","|  3|157cebd9-aaf0-475...|2022-03-02 17:23:58|80da8348-1707-403...|   fruit|         gold|      0.19|       4| 0.76|    e-wallet|\n","|  4|a81a6cd3-5e0c-44a...|2022-03-05 14:32:43|7f5e86e6-f06f-45f...|   fruit|        basic|      4.49|       2| 8.98|  debit card|\n","|  5|b5b3c8b9-f496-484...|2022-03-07 17:59:47|3bc6c1ea-0198-46d...|   fruit|     standard|      3.99|       4|15.96|        cash|\n","|  6|4997b1ae-f5aa-4b9...|2022-03-07 19:36:57|14736243-d346-438...|   fruit|     standard|      1.49|       4| 5.96|    e-wallet|\n","|  7|bfffee68-0736-42a...|2022-03-07 19:03:20|0ddc2379-adba-4fb...|   fruit|        basic|      3.99|       4|15.96| credit card|\n","|  8|ce50e984-90cd-4b4...|2022-03-07 11:34:32|ad81b46c-bf38-41c...|   fruit|   non-member|      3.99|       1| 3.99| credit card|\n","|  9|f0700cc9-e6f5-4b9...|2022-03-07 09:20:12|35e00193-aa27-412...|   fruit|      premium|      0.49|       3| 1.47|    e-wallet|\n","+---+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n","\n"]}],"source":["# a header is present in the data\n","spark.read.csv('/kaggle/input/cognizant-artificial-intelligence/sample_sales_data (1).csv',header=True).show(1)\n","\n","# automatically assign data types to columns\n","spark.read.csv('/kaggle/input/cognizant-artificial-intelligence/sample_sales_data (1).csv',header=True, inferSchema=True)\n","\n","# slightly differen format, specify the delimiter that splits columns\n","spark.read.option('delimiter', ',')\\\n","           .option('header', True)\\\n","           .option('interSchma',True)\\\n","            .csv('/kaggle/input/cognizant-artificial-intelligence/sample_sales_data (1).csv').show(5)\n","\n","# limit the number of loaded rows of data\n","spark.read.option('delimiter', ',')\\\n","           .option('header', True)\\\n","           .option('interSchma',True)\\\n","            .csv('/kaggle/input/cognizant-artificial-intelligence/sample_sales_data (1).csv')\\\n","            .limit(10).show()\n"]},{"cell_type":"markdown","id":"8a1946e4","metadata":{"papermill":{"duration":0.05278,"end_time":"2024-01-26T05:18:09.505099","exception":false,"start_time":"2024-01-26T05:18:09.452319","status":"completed"},"tags":[]},"source":["Cusomer schemas can be set by setting .csv(schema)"]},{"cell_type":"code","execution_count":8,"id":"4603b2a2","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:09.613412Z","iopub.status.busy":"2024-01-26T05:18:09.613037Z","iopub.status.idle":"2024-01-26T05:18:09.879018Z","shell.execute_reply":"2024-01-26T05:18:09.87793Z"},"papermill":{"duration":0.324333,"end_time":"2024-01-26T05:18:09.882744","exception":false,"start_time":"2024-01-26T05:18:09.558411","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+--------------------+----------+--------------------+--------+-------------+----------+--------+-----+------------+\n","|_c0|      transaction_id| timestamp|          product_id|category|customer_type|unit_price|quantity|total|patment_type|\n","+---+--------------------+----------+--------------------+--------+-------------+----------+--------+-----+------------+\n","|  0|a1c82654-c52c-45b...|2022-03-02|3bc6c1ea-0198-46d...|   fruit|         gold|      3.99|       2| 7.98|    e-wallet|\n","|  1|931ad550-09e8-4da...|2022-03-06|ad81b46c-bf38-41c...|   fruit|     standard|      3.99|       1| 3.99|    e-wallet|\n","|  2|ae133534-6f61-4cd...|2022-03-04|7c55cbd4-f306-4c0...|   fruit|      premium|      0.19|       2| 0.38|    e-wallet|\n","|  3|157cebd9-aaf0-475...|2022-03-02|80da8348-1707-403...|   fruit|         gold|      0.19|       4| 0.76|    e-wallet|\n","|  4|a81a6cd3-5e0c-44a...|2022-03-05|7f5e86e6-f06f-45f...|   fruit|        basic|      4.49|       2| 8.98|  debit card|\n","|  5|b5b3c8b9-f496-484...|2022-03-07|3bc6c1ea-0198-46d...|   fruit|     standard|      3.99|       4|15.96|        cash|\n","|  6|4997b1ae-f5aa-4b9...|2022-03-07|14736243-d346-438...|   fruit|     standard|      1.49|       4| 5.96|    e-wallet|\n","|  7|bfffee68-0736-42a...|2022-03-07|0ddc2379-adba-4fb...|   fruit|        basic|      3.99|       4|15.96| credit card|\n","|  8|ce50e984-90cd-4b4...|2022-03-07|ad81b46c-bf38-41c...|   fruit|   non-member|      3.99|       1| 3.99| credit card|\n","|  9|f0700cc9-e6f5-4b9...|2022-03-07|35e00193-aa27-412...|   fruit|      premium|      0.49|       3| 1.47|    e-wallet|\n","| 10|3c46e735-8cb6-4ec...|2022-03-06|ecac012c-1dec-41d...|   fruit|     standard|      4.99|       1| 4.99|        cash|\n","| 11|93c9eeb9-314d-4bd...|2022-03-01|3bc6c1ea-0198-46d...|   fruit|      premium|      3.99|       4|15.96|    e-wallet|\n","| 12|13049349-04ed-4df...|2022-03-05|ecac012c-1dec-41d...|   fruit|      premium|      4.99|       3|14.97|    e-wallet|\n","| 13|05bd411c-b512-49a...|2022-03-04|35e00193-aa27-412...|   fruit|      premium|      0.49|       4| 1.96|    e-wallet|\n","| 14|a016533b-7d7e-44b...|2022-03-01|04da844d-8dba-447...|   fruit|         gold|      0.49|       1| 0.49|        cash|\n","| 15|464b8d77-bc32-49d...|2022-03-05|0ddc2379-adba-4fb...|   fruit|      premium|      3.99|       4|15.96|    e-wallet|\n","| 16|cdadb465-b263-48a...|2022-03-01|14736243-d346-438...|   fruit|     standard|      1.49|       1| 1.49|  debit card|\n","| 17|1b5dcfd7-9e27-4fe...|2022-03-05|3bc6c1ea-0198-46d...|   fruit|     standard|      3.99|       3|11.97|    e-wallet|\n","| 18|32f8045e-7f73-4e8...|2022-03-01|35e00193-aa27-412...|   fruit|        basic|      0.49|       4| 1.96|  debit card|\n","| 19|e0940aa1-f237-49c...|2022-03-01|0ddc2379-adba-4fb...|   fruit|      premium|      3.99|       2| 7.98|    e-wallet|\n","+---+--------------------+----------+--------------------+--------+-------------+----------+--------+-----+------------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.types import DateType, StringType, FloatType, IntegerType, TimestampType\n","\n","# define the schema using StructType and StructField\n","schema = StructType([\n","    StructField(\"_c0\", IntegerType(), True),  # as the data has a , at the start of each row\n","    StructField(\"transaction_id\", StringType(), True),\n","    StructField(\"timestamp\", DateType(), True),  # read the column as a DateType,not TimestampType\n","    StructField(\"product_id\", StringType(), True),\n","    StructField(\"category\", StringType(), True),\n","    StructField(\"customer_type\",StringType(),True),\n","    StructField(\"unit_price\", FloatType(), True),\n","    StructField(\"quantity\", IntegerType(), True),\n","    StructField(\"total\", FloatType(), True),\n","    StructField(\"patment_type\", StringType(), True)\n","])\n","\n","df = spark.read.csv('/kaggle/input/cognizant-artificial-intelligence/sample_sales_data (1).csv',header=True,inferSchema=False,schema=schema)\n","df.show()"]},{"cell_type":"markdown","id":"c32221a1","metadata":{"papermill":{"duration":0.060275,"end_time":"2024-01-26T05:18:10.021464","exception":false,"start_time":"2024-01-26T05:18:09.961189","status":"completed"},"tags":[]},"source":["# Knowing your Pyspark Types"]},{"cell_type":"markdown","id":"e12393d2","metadata":{"papermill":{"duration":0.05344,"end_time":"2024-01-26T05:18:10.130695","exception":false,"start_time":"2024-01-26T05:18:10.077255","status":"completed"},"tags":[]},"source":["To set **StructField** and define a type, we should know which types are available to us in pyspark."]},{"cell_type":"markdown","id":"6428a9a2","metadata":{"papermill":{"duration":0.055389,"end_time":"2024-01-26T05:18:10.239158","exception":false,"start_time":"2024-01-26T05:18:10.183769","status":"completed"},"tags":[]},"source":["* 1.StringType:represents string values.\n","* 2.IntegerType:represents integer values.\n","* 3.LongType:represents long integer values.\n","* 4.FloatType:represents float values.\n","* 5.DoubleType:represents double values.\n","* 6.BooleanType:represents boolean values.\n","* 7.DateType:represents date values.\n","* 8.TimestampType:represents timestamp values.\n","* 9.ArrayType:represents arrays of elements with a specific data type.\n","* 10.MapType:represents key-values pairs with specific data types for keys and values.\n","* 11.StructType:represents a structure or record with multiple fields."]},{"cell_type":"code","execution_count":9,"id":"202b36e8","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:10.350234Z","iopub.status.busy":"2024-01-26T05:18:10.34982Z","iopub.status.idle":"2024-01-26T05:18:10.40648Z","shell.execute_reply":"2024-01-26T05:18:10.405303Z"},"papermill":{"duration":0.114682,"end_time":"2024-01-26T05:18:10.409633","exception":false,"start_time":"2024-01-26T05:18:10.294951","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["DataFrame[id: bigint, info: map<string,string>]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","from pyspark.sql.types import MapType, StringType\n","\n","# Create a sparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create a DataFrame with a column of MapType\n","data = [(1,{\"name\": \"John\", \"age\": \"30\"}),\n","        (2,{\"name\": \"Jane\", \"age\": \"25\"})\n","]\n","\n","df = spark.createDataFrame(data,[\"id\", \"info\"])\n","df"]},{"cell_type":"code","execution_count":10,"id":"72f5038a","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:10.527029Z","iopub.status.busy":"2024-01-26T05:18:10.526605Z","iopub.status.idle":"2024-01-26T05:18:10.986622Z","shell.execute_reply":"2024-01-26T05:18:10.985738Z"},"papermill":{"duration":0.520809,"end_time":"2024-01-26T05:18:10.989562","exception":false,"start_time":"2024-01-26T05:18:10.468753","status":"completed"},"tags":[]},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.types import ArrayType, StringType\n","\n","# create a SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# sampledata\n","data = [\n","    (\"Alice\", [\"apple\", \"banana\", \"orange\"]),\n","    (\"Bob\", [\"grape\", \"kiwi\"]),\n","    (\"Charlie\", [\"watermelon\"])\n","]\n","\n","# define the schema with ArrayType\n","spark.createDataFrame(data, [\"name\", \"fruits\"])\n","\n","spark.stop()"]},{"cell_type":"markdown","id":"1bd2b682","metadata":{"papermill":{"duration":0.055092,"end_time":"2024-01-26T05:18:11.098812","exception":false,"start_time":"2024-01-26T05:18:11.04372","status":"completed"},"tags":[]},"source":["# TimeStamp Zone Consideration"]},{"cell_type":"markdown","id":"1f1629cb","metadata":{"papermill":{"duration":0.05402,"end_time":"2024-01-26T05:18:11.206863","exception":false,"start_time":"2024-01-26T05:18:11.152843","status":"completed"},"tags":[]},"source":["If your column is of type datatime(TimestampType), here is how you can use it with different timezones, so you can make the necessary adjustments if needed."]},{"cell_type":"code","execution_count":11,"id":"f38ec569","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:11.318914Z","iopub.status.busy":"2024-01-26T05:18:11.317652Z","iopub.status.idle":"2024-01-26T05:18:14.712175Z","shell.execute_reply":"2024-01-26T05:18:14.711021Z"},"papermill":{"duration":3.453765,"end_time":"2024-01-26T05:18:14.715718","exception":false,"start_time":"2024-01-26T05:18:11.261953","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------------------+-------------------+---------------------+\n","|          timestamp|  timestamp_with_ny|timestamp_with_moscow|\n","+-------------------+-------------------+---------------------+\n","|2022-01-01 12:00:00|2022-01-01 07:00:00|  2022-01-01 15:00:00|\n","+-------------------+-------------------+---------------------+\n","\n","+-------------------+-------------------+---------------------+--------------------+\n","|          timestamp|  timestamp_with_ny|timestamp_with_moscow|timestamp_utc_moscow|\n","+-------------------+-------------------+---------------------+--------------------+\n","|2022-01-01 12:00:00|2022-01-01 07:00:00|  2022-01-01 15:00:00| 2022-01-01 12:00:00|\n","+-------------------+-------------------+---------------------+--------------------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import from_utc_timestamp, to_utc_timestamp\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create a DateFrame with a timestamp column (UTC time)\n","data = [(\"2022-01-01 12:00:00\",)]\n","df = spark.createDataFrame(data, [\"timestamp\"])\n","\n","# if timestamp is UTC\n","df_tz = df.withColumn(\"timestamp_with_ny\", from_utc_timestamp(df.timestamp,\"America/New_York\"))\n","df_tz = df_tz.withColumn(\"timestamp_with_moscow\", from_utc_timestamp(df.timestamp, \"Europe/Moscow\"))\n","df_tz.show()\n","\n","# if timestamp os local\n","df_utc = df_tz.withColumn(\"timestamp_utc_ny\", to_utc_timestamp(df_tz.timestamp_with_ny, \"America/New_York\"))\n","df_utc = df_tz.withColumn(\"timestamp_utc_moscow\", to_utc_timestamp(df_tz.timestamp_with_moscow, \"Europe/Moscow\"))\n","df_utc.show()"]},{"cell_type":"markdown","id":"20bf9d81","metadata":{"papermill":{"duration":0.059689,"end_time":"2024-01-26T05:18:14.860503","exception":false,"start_time":"2024-01-26T05:18:14.800814","status":"completed"},"tags":[]},"source":["# Making a linear model"]},{"cell_type":"markdown","id":"e3ba6e46","metadata":{"papermill":{"duration":0.055036,"end_time":"2024-01-26T05:18:14.971453","exception":false,"start_time":"2024-01-26T05:18:14.916417","status":"completed"},"tags":[]},"source":["The process of creating models differs a little bit to how one would go about it in sklearn. Once we have a dataframe that contains all of our features & target variable df, we need to assemble them into a vectorised format using VectorAssemble, to do so we need to define the inputCols and outputCol(whoch will assemble all our input feature data). Loading the relevant model from the library pyspark.ml, we then need to define inputCols(which is the output column of the VectorAssemble) and outputCol arguments.\n","\n","Initialised the model(LinearRegression), we call the method fit and define it as a variable(which is different to sklearn). To use the model for prediction, we need to transform the new data into the same vectorised format using the assemble to create new_data, and use model.transform(new_data) to make the prediction."]},{"cell_type":"code","execution_count":12,"id":"df436aa4","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:15.087262Z","iopub.status.busy":"2024-01-26T05:18:15.086848Z","iopub.status.idle":"2024-01-26T05:18:22.361548Z","shell.execute_reply":"2024-01-26T05:18:22.360476Z"},"papermill":{"duration":7.336629,"end_time":"2024-01-26T05:18:22.36433","exception":false,"start_time":"2024-01-26T05:18:15.027701","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+--------+--------+----------+----------+\n","|feature1|feature2|  features|prediction|\n","+--------+--------+----------+----------+\n","|       6|      12|[6.0,12.0]|      18.0|\n","+--------+--------+----------+----------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.ml.regression import LinearRegression\n","from pyspark.ml.feature import VectorAssembler\n","\n","# Creaete a SparkSsession\n","spark = SparkSession.builder.getOrCreate()\n","spark.sparkContext.setLogLevel(\"ERROR\")\n","\n","# Sample dataset (two features & target variable)\n","data = [\n","    (1,2,3),\n","    (2,4,6),\n","    (3,6,9),\n","    (4,8,12),\n","    (5,10,15)\n","]\n","df = spark.createDataFrame(data, [\"feature1\", \"feature2\", \"target\"])\n","\n","# Prepare the data for modeling\n","assembler = VectorAssembler(inputCols=[\"feature1\",\"feature2\"], outputCol=\"features\")\n","df = assembler.transform(df)\n","df\n","\n","# Create and fit the linear regression model\n","lr = LinearRegression(featuresCol = \"features\", labelCol = \"target\")\n","model = lr.fit(df)\n","\n","# Make predictions on new data\n","new_data = spark.createDataFrame([(6,12)],[\"feature1\", \"feature2\"])\n","new_data = assembler.transform(new_data)\n","predictions = model.transform(new_data)\n","predictions.show()\n","spark.stop()"]},{"cell_type":"markdown","id":"f0d77aa2","metadata":{"papermill":{"duration":0.05624,"end_time":"2024-01-26T05:18:22.47656","exception":false,"start_time":"2024-01-26T05:18:22.42032","status":"completed"},"tags":[]},"source":["# Filter rows that contain item in array column"]},{"cell_type":"markdown","id":"a5584a11","metadata":{"papermill":{"duration":0.054505,"end_time":"2024-01-26T05:18:22.586194","exception":false,"start_time":"2024-01-26T05:18:22.531689","status":"completed"},"tags":[]},"source":["PySpark contains a special function array_contains which allows you to check if a specifield value exists in an array column. It returns a boolean value indicating whether the array contains the specified value."]},{"cell_type":"code","execution_count":13,"id":"cc0e9704","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:22.69792Z","iopub.status.busy":"2024-01-26T05:18:22.697483Z","iopub.status.idle":"2024-01-26T05:18:24.829172Z","shell.execute_reply":"2024-01-26T05:18:24.828044Z"},"papermill":{"duration":2.191324,"end_time":"2024-01-26T05:18:24.832551","exception":false,"start_time":"2024-01-26T05:18:22.641227","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-----+--------------------+\n","| name|              fruits|\n","+-----+--------------------+\n","|Alice|[apple, banana, o...|\n","+-----+--------------------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import array_contains\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.appName(\"filter_rows\").getOrCreate()\n","\n","# Example of ArrayType\n","data = [\n","    (\"Alice\", [\"apple\", \"banana\", \"orange\"]),\n","    (\"Bob\", [\"grape\", \"kiwi\"]),\n","    (\"Charlie\", [\"watermelon\"])\n","]\n","\n","# Define the schema with ArrayType\n","df = spark.createDataFrame(data, [\"name\", \"fruits\"])\n","# DateFrame[name:string, fruits:array<string>]\n","\n","# Fliter rows where the array column contains a specific element\n","filtered_df = df.where(array_contains(df.fruits, \"orange\"))\n","\n","# Show the filtered DataFrame\n","filtered_df.show()"]},{"cell_type":"code","execution_count":14,"id":"fe8010a6","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:24.946426Z","iopub.status.busy":"2024-01-26T05:18:24.945993Z","iopub.status.idle":"2024-01-26T05:18:26.61207Z","shell.execute_reply":"2024-01-26T05:18:26.611019Z"},"papermill":{"duration":1.726638,"end_time":"2024-01-26T05:18:26.615056","exception":false,"start_time":"2024-01-26T05:18:24.888418","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+--------------------+--------+\n","|   name|              fruits|contains|\n","+-------+--------------------+--------+\n","|  Alice|[apple, banana, o...|    true|\n","|    Bob|       [grape, kiwi]|   false|\n","|Charlie|        [watermelon]|   false|\n","+-------+--------------------+--------+\n","\n"]}],"source":["test = df.withColumn('contains',array_contains(df.fruits,\"orange\")).show()\n","spark.stop()"]},{"cell_type":"markdown","id":"ee1a6c45","metadata":{"papermill":{"duration":0.054804,"end_time":"2024-01-26T05:18:26.726038","exception":false,"start_time":"2024-01-26T05:18:26.671234","status":"completed"},"tags":[]},"source":["# SQL like functions(SELECT)"]},{"cell_type":"markdown","id":"c19bff38","metadata":{"papermill":{"duration":0.053855,"end_time":"2024-01-26T05:18:26.835571","exception":false,"start_time":"2024-01-26T05:18:26.781716","status":"completed"},"tags":[]},"source":["Select columns from Pyspark DataFrame, similar to SELECT in SQL"]},{"cell_type":"code","execution_count":15,"id":"f5e69427","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:26.950595Z","iopub.status.busy":"2024-01-26T05:18:26.950217Z","iopub.status.idle":"2024-01-26T05:18:29.068315Z","shell.execute_reply":"2024-01-26T05:18:29.067183Z"},"papermill":{"duration":2.180846,"end_time":"2024-01-26T05:18:29.072423","exception":false,"start_time":"2024-01-26T05:18:26.891577","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------+---+-------------+\n","|   Name|Age|         City|\n","+-------+---+-------------+\n","|  Alice| 25|     New York|\n","|    Bob| 30|  Los Angeles|\n","|Charlie| 35|San Francisco|\n","+-------+---+-------------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create a DataFrame\n","data = [\n","    (\"Alice\", 25, \"New York\"),\n","    (\"Bob\", 30, \"Los Angeles\"),\n","    (\"Charlie\", 35, \"San Francisco\")\n","]\n","\n","df = spark.createDataFrame(data, [\"Name\", \"Age\", \"City\"])\n","df.show()"]},{"cell_type":"code","execution_count":16,"id":"80e4cf83","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:29.197621Z","iopub.status.busy":"2024-01-26T05:18:29.197209Z","iopub.status.idle":"2024-01-26T05:18:30.825407Z","shell.execute_reply":"2024-01-26T05:18:30.824313Z"},"papermill":{"duration":1.693653,"end_time":"2024-01-26T05:18:30.828062","exception":false,"start_time":"2024-01-26T05:18:29.134409","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-------------+\n","|   Name|         City|\n","+-------+-------------+\n","|  Alice|     New York|\n","|    Bob|  Los Angeles|\n","|Charlie|San Francisco|\n","+-------+-------------+\n","\n"]}],"source":["# Select specific columns from the DataFrame\n","selected_df = df.select(\"Name\", \"City\")\n","selected_df.show()\n","spark.stop()"]},{"cell_type":"markdown","id":"99e09c95","metadata":{"papermill":{"duration":0.056523,"end_time":"2024-01-26T05:18:30.94046","exception":false,"start_time":"2024-01-26T05:18:30.883937","status":"completed"},"tags":[]},"source":["# SQL like functions(WHERE)"]},{"cell_type":"markdown","id":"e7f74cd0","metadata":{"papermill":{"duration":0.055458,"end_time":"2024-01-26T05:18:31.05202","exception":false,"start_time":"2024-01-26T05:18:30.996562","status":"completed"},"tags":[]},"source":["Filter rows in Pyspark DataFrame, similar to WHERE in SQL"]},{"cell_type":"code","execution_count":17,"id":"d84560e8","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:31.165381Z","iopub.status.busy":"2024-01-26T05:18:31.164957Z","iopub.status.idle":"2024-01-26T05:18:33.258903Z","shell.execute_reply":"2024-01-26T05:18:33.257793Z"},"papermill":{"duration":2.155703,"end_time":"2024-01-26T05:18:33.262883","exception":false,"start_time":"2024-01-26T05:18:31.10718","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------+---+-------------+\n","|   Name|Age|         City|\n","+-------+---+-------------+\n","|  Alice| 25|     New York|\n","|    Bob| 30|  Los Angeles|\n","|Charlie| 35|San Francisco|\n","+-------+---+-------------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create a DataFrame\n","data = [\n","    (\"Alice\", 25, \"New York\"),\n","    (\"Bob\", 30, \"Los Angeles\"),\n","    (\"Charlie\", 35, \"San Francisco\")\n","]\n","\n","df = spark.createDataFrame(data, [\"Name\", \"Age\", \"City\"])\n","df.show()"]},{"cell_type":"code","execution_count":18,"id":"222f0e33","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:33.386769Z","iopub.status.busy":"2024-01-26T05:18:33.38638Z","iopub.status.idle":"2024-01-26T05:18:35.086496Z","shell.execute_reply":"2024-01-26T05:18:35.08535Z"},"papermill":{"duration":1.759405,"end_time":"2024-01-26T05:18:35.089197","exception":false,"start_time":"2024-01-26T05:18:33.329792","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+---+-------------+\n","|   Name|Age|         City|\n","+-------+---+-------------+\n","|Charlie| 35|San Francisco|\n","+-------+---+-------------+\n","\n"]}],"source":["from pyspark.sql import functions as f\n","\n","filtered_df = df.filter((f.col('Age')>30) | (df.Age == 'Charlie'))\n","\n","filtered_df.show()\n","spark.stop()"]},{"cell_type":"markdown","id":"9e5fb53d","metadata":{"papermill":{"duration":0.055312,"end_time":"2024-01-26T05:18:35.202668","exception":false,"start_time":"2024-01-26T05:18:35.147356","status":"completed"},"tags":[]},"source":["# SQL like functions(GROUP BY)"]},{"cell_type":"markdown","id":"1a0136d8","metadata":{"papermill":{"duration":0.054949,"end_time":"2024-01-26T05:18:35.314007","exception":false,"start_time":"2024-01-26T05:18:35.259058","status":"completed"},"tags":[]},"source":["Sample single column based group by operations with agg functionality options"]},{"cell_type":"code","execution_count":19,"id":"e8b18327","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:35.427401Z","iopub.status.busy":"2024-01-26T05:18:35.426949Z","iopub.status.idle":"2024-01-26T05:18:37.464299Z","shell.execute_reply":"2024-01-26T05:18:37.46314Z"},"papermill":{"duration":2.09694,"end_time":"2024-01-26T05:18:37.466816","exception":false,"start_time":"2024-01-26T05:18:35.369876","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------+---+--------+\n","|   name|age|    city|\n","+-------+---+--------+\n","|  Alice| 25|New York|\n","|    Bob| 30|  London|\n","|Charlie| 35|New York|\n","|   Dave| 40|  London|\n","+-------+---+--------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create a DataFrame\n","data = [\n","    (\"Alice\", 25, \"New York\"),\n","    (\"Bob\", 30, \"London\"),\n","    (\"Charlie\", 35, \"New York\"),\n","    (\"Dave\", 40, \"London\")\n","]\n","\n","df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"])\n","df.show()"]},{"cell_type":"code","execution_count":20,"id":"71dc7b26","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:37.580756Z","iopub.status.busy":"2024-01-26T05:18:37.580114Z","iopub.status.idle":"2024-01-26T05:18:39.641103Z","shell.execute_reply":"2024-01-26T05:18:39.639859Z"},"papermill":{"duration":2.12113,"end_time":"2024-01-26T05:18:39.644198","exception":false,"start_time":"2024-01-26T05:18:37.523068","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+--------+-----------+----------+\n","|    city|average_age|name_count|\n","+--------+-----------+----------+\n","|New York|       30.0|         2|\n","|  London|       35.0|         2|\n","+--------+-----------+----------+\n","\n"]}],"source":["from pyspark.sql.functions import avg, count, expr\n","\n","# Group the DataFrame by the 'city' column\n","grouped_df = df.groupBy(\"city\")\n","\n","# giving alias\n","result = grouped_df.agg(avg(df.age).alias(\"average_age\"),\n","                       count(df.name).alias(\"name_count\"))\n","\n","# Show the result\n","result.show()\n","spark.stop()"]},{"cell_type":"markdown","id":"61ba2675","metadata":{"papermill":{"duration":0.055349,"end_time":"2024-01-26T05:18:39.755092","exception":false,"start_time":"2024-01-26T05:18:39.699743","status":"completed"},"tags":[]},"source":["# SQL like functions(ORDER BY)"]},{"cell_type":"markdown","id":"17aadf33","metadata":{"papermill":{"duration":0.055408,"end_time":"2024-01-26T05:18:39.867866","exception":false,"start_time":"2024-01-26T05:18:39.812458","status":"completed"},"tags":[]},"source":["Ordering a column using orderBy based on ascending or descending order using asc or desc together with col function"]},{"cell_type":"code","execution_count":21,"id":"307082b8","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:39.984691Z","iopub.status.busy":"2024-01-26T05:18:39.983629Z","iopub.status.idle":"2024-01-26T05:18:42.082737Z","shell.execute_reply":"2024-01-26T05:18:42.081371Z"},"papermill":{"duration":2.160122,"end_time":"2024-01-26T05:18:42.085251","exception":false,"start_time":"2024-01-26T05:18:39.925129","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------+---+------+\n","|   name|age|height|\n","+-------+---+------+\n","|  Alice| 25|   180|\n","|    Bob| 25|   150|\n","|Charlie| 35|   167|\n","+-------+---+------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create a DataFrame\n","data = [\n","    (\"Alice\",25,180),\n","    (\"Bob\", 25,150),\n","    (\"Charlie\",35,167)\n","]\n","df = spark.createDataFrame(data,[\"name\", \"age\",\"height\"])\n","df.show()"]},{"cell_type":"code","execution_count":22,"id":"53469f2b","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:42.200821Z","iopub.status.busy":"2024-01-26T05:18:42.200428Z","iopub.status.idle":"2024-01-26T05:18:42.827072Z","shell.execute_reply":"2024-01-26T05:18:42.825957Z"},"papermill":{"duration":0.68764,"end_time":"2024-01-26T05:18:42.829838","exception":false,"start_time":"2024-01-26T05:18:42.142198","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+---+------+\n","|   name|age|height|\n","+-------+---+------+\n","|Charlie| 35|   167|\n","|    Bob| 25|   150|\n","|  Alice| 25|   180|\n","+-------+---+------+\n","\n"]}],"source":["# Order DataFrame by age in descending order\n","df.orderBy(f.col('age').desc(),f.col('height').asc()).show()"]},{"cell_type":"code","execution_count":23,"id":"3314dc8c","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:42.946396Z","iopub.status.busy":"2024-01-26T05:18:42.94598Z","iopub.status.idle":"2024-01-26T05:18:44.476972Z","shell.execute_reply":"2024-01-26T05:18:44.475954Z"},"papermill":{"duration":1.592656,"end_time":"2024-01-26T05:18:44.479805","exception":false,"start_time":"2024-01-26T05:18:42.887149","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+---+------+\n","|person_name|age|height|\n","+-----------+---+------+\n","|    Charlie| 35|   167|\n","|        Bob| 25|   150|\n","|      Alice| 25|   180|\n","+-----------+---+------+\n","\n"]}],"source":["# Rename the \"name\" column to \"person_name\" and order the DataFrame by \"age\" column in descending order\n","result = df.withColumnRenamed(\"name\",\"person_name\").orderBy(col(\"age\").desc())\n","\n","# Show the result\n","result.show()\n","spark.stop()"]},{"cell_type":"markdown","id":"93f8f2de","metadata":{"papermill":{"duration":0.057781,"end_time":"2024-01-26T05:18:44.595123","exception":false,"start_time":"2024-01-26T05:18:44.537342","status":"completed"},"tags":[]},"source":["# SQL like functions(JOIN)"]},{"cell_type":"markdown","id":"602ab91f","metadata":{"papermill":{"duration":0.056813,"end_time":"2024-01-26T05:18:44.709689","exception":false,"start_time":"2024-01-26T05:18:44.652876","status":"completed"},"tags":[]},"source":["* Using **pyspark.sql**, we can join dataframes with the notation shown in previous this article, however pyspark dataframe has its own method for jioning fataframe table **join()**\n","\n","* As with the pandas notation of **merge** df1.merge(df2), we can join dataframes using the similar notation **df1.join(df2,'on','how')**"]},{"cell_type":"code","execution_count":24,"id":"aa14a9fb","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:44.8289Z","iopub.status.busy":"2024-01-26T05:18:44.828492Z","iopub.status.idle":"2024-01-26T05:18:47.506635Z","shell.execute_reply":"2024-01-26T05:18:47.505407Z"},"papermill":{"duration":2.741228,"end_time":"2024-01-26T05:18:47.510435","exception":false,"start_time":"2024-01-26T05:18:44.769207","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+---+-----+\n","| id| name|\n","+---+-----+\n","|  1| John|\n","|  2|Alice|\n","|  3|  Bob|\n","+---+-----+\n","\n","+---+---+\n","| id|age|\n","+---+---+\n","|  1| 25|\n","|  2| 30|\n","|  4| 35|\n","+---+---+\n","\n"]}],"source":["spark = SparkSession.builder.getOrCreate()\n","\n","# Input dataframes\n","df1 = spark.createDataFrame([(1,\"John\"), (2,\"Alice\"), (3, \"Bob\")], [\"id\", \"name\"])\n","df2 = spark.createDataFrame([(1,25), (2,30), (4,35)], [\"id\", \"age\"])\n","df1.show()\n","df2.show()"]},{"cell_type":"code","execution_count":25,"id":"0e7063ee","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:47.660592Z","iopub.status.busy":"2024-01-26T05:18:47.66011Z","iopub.status.idle":"2024-01-26T05:18:49.007961Z","shell.execute_reply":"2024-01-26T05:18:49.006573Z"},"papermill":{"duration":1.422491,"end_time":"2024-01-26T05:18:49.011105","exception":false,"start_time":"2024-01-26T05:18:47.588614","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 5:>                                                          (0 + 4) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["+---+-----+---+\n","| id| name|age|\n","+---+-----+---+\n","|  1| John| 25|\n","|  2|Alice| 30|\n","+---+-----+---+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["joined_df = df1.join(df2, \"id\", \"inner\")\n","joined_df.show()"]},{"cell_type":"code","execution_count":26,"id":"efba9c71","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:49.140994Z","iopub.status.busy":"2024-01-26T05:18:49.140462Z","iopub.status.idle":"2024-01-26T05:18:50.399092Z","shell.execute_reply":"2024-01-26T05:18:50.397786Z"},"papermill":{"duration":1.32141,"end_time":"2024-01-26T05:18:50.401784","exception":false,"start_time":"2024-01-26T05:18:49.080374","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+---+-----+----+\n","| id| name| age|\n","+---+-----+----+\n","|  1| John|  25|\n","|  2|Alice|  30|\n","|  3|  Bob|NULL|\n","+---+-----+----+\n","\n"]}],"source":["# left join\n","joined_df = df1.join(df2, \"id\", \"left\")\n","joined_df.show()"]},{"cell_type":"code","execution_count":27,"id":"94b93a7f","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:50.522178Z","iopub.status.busy":"2024-01-26T05:18:50.521718Z","iopub.status.idle":"2024-01-26T05:18:52.569192Z","shell.execute_reply":"2024-01-26T05:18:52.568095Z"},"papermill":{"duration":2.110309,"end_time":"2024-01-26T05:18:52.572669","exception":false,"start_time":"2024-01-26T05:18:50.46236","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+---+-----+----+\n","| id| name| age|\n","+---+-----+----+\n","|  1| John|  25|\n","|  2|Alice|  30|\n","|  3|  Bob|NULL|\n","|  4| NULL|  35|\n","+---+-----+----+\n","\n"]}],"source":["# Full outer join\n","joined_df = df1.join(df2, \"id\", \"outer\")\n","joined_df.show()\n","spark.stop()"]},{"cell_type":"markdown","id":"8b2086f8","metadata":{"papermill":{"duration":0.062662,"end_time":"2024-01-26T05:18:52.704988","exception":false,"start_time":"2024-01-26T05:18:52.642326","status":"completed"},"tags":[]},"source":["And another example with data "]},{"cell_type":"code","execution_count":28,"id":"01fa144d","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:52.830772Z","iopub.status.busy":"2024-01-26T05:18:52.83039Z","iopub.status.idle":"2024-01-26T05:18:55.579447Z","shell.execute_reply":"2024-01-26T05:18:55.578234Z"},"papermill":{"duration":2.814104,"end_time":"2024-01-26T05:18:55.583149","exception":false,"start_time":"2024-01-26T05:18:52.769045","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+----------+------+\n","|      date|value1|\n","+----------+------+\n","|2021-01-01|    10|\n","|2021-01-02|    20|\n","|2021-01-03|    30|\n","|2021-01-04|    70|\n","+----------+------+\n","\n","+----------+------+\n","|      date|value2|\n","+----------+------+\n","|2021-01-01|    40|\n","|2021-01-02|    50|\n","|2021-01-03|    60|\n","|2021-01-04|    70|\n","+----------+------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","\n","# Create a spark session\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create sample data for dataset1(January)\n","dataset1 = spark.createDataFrame([\n","    (\"2021-01-01\", 10),\n","    (\"2021-01-02\", 20),\n","    (\"2021-01-03\", 30),\n","    (\"2021-01-04\", 70)\n","], [\"date\", \"value1\"])\n","\n","# Create sample data for dataset2(February)\n","dataset2 = spark.createDataFrame([\n","    (\"2021-01-01\", 40),\n","    (\"2021-01-02\", 50),\n","    (\"2021-01-03\", 60),\n","    (\"2021-01-04\", 70)\n","], [\"date\", \"value2\"])\n","\n","# Show the joined dataframe\n","dataset1.show()\n","dataset2.show()"]},{"cell_type":"code","execution_count":29,"id":"d77a9ee8","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:55.716857Z","iopub.status.busy":"2024-01-26T05:18:55.716332Z","iopub.status.idle":"2024-01-26T05:18:56.806678Z","shell.execute_reply":"2024-01-26T05:18:56.805484Z"},"papermill":{"duration":1.161148,"end_time":"2024-01-26T05:18:56.809622","exception":false,"start_time":"2024-01-26T05:18:55.648474","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 5:=============================>                             (2 + 2) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["+----------+------+------+\n","|      date|value1|value2|\n","+----------+------+------+\n","|2021-01-01|    10|    40|\n","|2021-01-02|    20|    50|\n","|2021-01-03|    30|    60|\n","|2021-01-04|    70|    70|\n","+----------+------+------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["dataset1.join(dataset2,on='date', how='outer').show()"]},{"cell_type":"markdown","id":"f3f90904","metadata":{"papermill":{"duration":0.060702,"end_time":"2024-01-26T05:18:56.930425","exception":false,"start_time":"2024-01-26T05:18:56.869723","status":"completed"},"tags":[]},"source":["# PySpark UDF"]},{"cell_type":"markdown","id":"51d0c16c","metadata":{"papermill":{"duration":0.059376,"end_time":"2024-01-26T05:18:57.050798","exception":false,"start_time":"2024-01-26T05:18:56.991422","status":"completed"},"tags":[]},"source":["PySpark UDFs are custom functions that can be created and applied to DataFrame columns in PySpark. They allow users to perform custom computations or transformations on DataFrame or transformations on DataFrame databy defing their own functions and applying them to specific columns.\n","\n","In this example we will create a **UDF** called **square** and use it to create a new column, which will include the data fro a single column of data."]},{"cell_type":"code","execution_count":30,"id":"fff6c732","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:57.170343Z","iopub.status.busy":"2024-01-26T05:18:57.169945Z","iopub.status.idle":"2024-01-26T05:18:59.531158Z","shell.execute_reply":"2024-01-26T05:18:59.529852Z"},"papermill":{"duration":2.425865,"end_time":"2024-01-26T05:18:59.535483","exception":false,"start_time":"2024-01-26T05:18:57.109618","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------+---+------+\n","|   name|age|square|\n","+-------+---+------+\n","|  Alice| 25|   625|\n","|    Bob| 30|   900|\n","|Charlie| 35|  1225|\n","+-------+---+------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["from pyspark.sql.functions import udf\n","from pyspark.sql.types import IntegerType\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create a DataFrame\n","data = [\n","    (\"Alice\",25),\n","    (\"Bob\", 30),\n","    (\"Charlie\", 35)\n","]\n","df = spark.createDataFrame(data,[\"name\",\"age\"])\n","\n","def square(num):\n","    return num*num\n","\n","# register UDF\n","square_udf = udf(square, IntegerType())\n","\n","# Apply the UDF to a DataFrame column\n","new_df = df.withColumn(\"square\", square_udf(df[\"age\"]))\n","new_df.show()"]},{"cell_type":"markdown","id":"33b10c5d","metadata":{"papermill":{"duration":0.058834,"end_time":"2024-01-26T05:18:59.674314","exception":false,"start_time":"2024-01-26T05:18:59.61548","status":"completed"},"tags":[]},"source":["UDF via **lambda functions**"]},{"cell_type":"code","execution_count":31,"id":"68ecac34","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:18:59.794529Z","iopub.status.busy":"2024-01-26T05:18:59.793642Z","iopub.status.idle":"2024-01-26T05:19:00.670654Z","shell.execute_reply":"2024-01-26T05:19:00.668085Z"},"papermill":{"duration":0.941501,"end_time":"2024-01-26T05:19:00.674908","exception":false,"start_time":"2024-01-26T05:18:59.733407","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+---+------+\n","|   name|age|added2|\n","+-------+---+------+\n","|  Alice| 25|    50|\n","|    Bob| 30|    60|\n","|Charlie| 35|    70|\n","+-------+---+------+\n","\n"]}],"source":["sum_udf = udf(lambda x,y : x+y, IntegerType())\n","df = df.withColumn(\"added2\", sum_udf(df['age'],df['age']))\n","df.show()"]},{"cell_type":"markdown","id":"0d38949b","metadata":{"papermill":{"duration":0.060155,"end_time":"2024-01-26T05:19:00.79822","exception":false,"start_time":"2024-01-26T05:19:00.738065","status":"completed"},"tags":[]},"source":["**UDF** can be used with multiple columns"]},{"cell_type":"code","execution_count":32,"id":"56b23112","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:00.9208Z","iopub.status.busy":"2024-01-26T05:19:00.9204Z","iopub.status.idle":"2024-01-26T05:19:02.211732Z","shell.execute_reply":"2024-01-26T05:19:02.210007Z"},"papermill":{"duration":1.35751,"end_time":"2024-01-26T05:19:02.215527","exception":false,"start_time":"2024-01-26T05:19:00.858017","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 14:======================================>                   (2 + 1) / 3]\r"]},{"name":"stdout","output_type":"stream","text":["+-------+---+------+------+\n","|   name|age|added2|added3|\n","+-------+---+------+------+\n","|  Alice| 25|    50|  1250|\n","|    Bob| 30|    60|  1800|\n","|Charlie| 35|    70|  2450|\n","+-------+---+------+------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["def square(num1,num2):\n","    return num1*num2\n","\n","multiply_udf = udf(square,IntegerType())\n","\n","df = df.withColumn(\"added3\",multiply_udf(df['age'],df['added2']))\n","df.show()"]},{"cell_type":"markdown","id":"b2cc5860","metadata":{"papermill":{"duration":0.059629,"end_time":"2024-01-26T05:19:02.34876","exception":false,"start_time":"2024-01-26T05:19:02.289131","status":"completed"},"tags":[]},"source":["using UDF with a consetant, we need to utilise the function **lit()**"]},{"cell_type":"code","execution_count":33,"id":"f082bcba","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:02.468789Z","iopub.status.busy":"2024-01-26T05:19:02.468369Z","iopub.status.idle":"2024-01-26T05:19:04.928686Z","shell.execute_reply":"2024-01-26T05:19:04.927507Z"},"papermill":{"duration":2.523828,"end_time":"2024-01-26T05:19:04.931603","exception":false,"start_time":"2024-01-26T05:19:02.407775","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------+---+------+------+------+\n","|   name|age|added2|added3|added4|\n","+-------+---+------+------+------+\n","|  Alice| 25|    50|  1250|   100|\n","|    Bob| 30|    60|  1800|   120|\n","|Charlie| 35|    70|  2450|   140|\n","+-------+---+------+------+------+\n","\n"]}],"source":["from pyspark.sql.functions import lit\n","\n","def constant(num1,num2):\n","    return num1 * num2\n","\n","multiply_udf = udf(constant,IntegerType())\n","\n","df = df.withColumn(\"added4\",multiply_udf(df['age'],lit(4)))\n","df.show()\n","spark.stop()"]},{"cell_type":"markdown","id":"0d37863a","metadata":{"papermill":{"duration":0.060627,"end_time":"2024-01-26T05:19:05.053463","exception":false,"start_time":"2024-01-26T05:19:04.992836","status":"completed"},"tags":[]},"source":["UDF can also be used with arraytypes"]},{"cell_type":"code","execution_count":34,"id":"30e561ad","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:05.175012Z","iopub.status.busy":"2024-01-26T05:19:05.174278Z","iopub.status.idle":"2024-01-26T05:19:08.841296Z","shell.execute_reply":"2024-01-26T05:19:08.840075Z"},"papermill":{"duration":3.73141,"end_time":"2024-01-26T05:19:08.844957","exception":false,"start_time":"2024-01-26T05:19:05.113547","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+---+------------+-------+\n","| id|     numbers|average|\n","+---+------------+-------+\n","|  1|[10, 20, 30]|   20.0|\n","|  2|[15, 25, 35]|   25.0|\n","|  3|[12, 22, 32]|   22.0|\n","+---+------------+-------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import DoubleType\n","\n","# Create a Spark session\n","spark = SparkSession.builder.appName(\"UDF Example\").getOrCreate()\n","spark.sparkContext.setLogLevel(\"ERROR\")\n","\n","# Sample data\n","data = [\n","    (1,[10,20,30]),\n","    (2,[15,25,35]),\n","    (3,[12,22,32])\n","]\n","df = spark.createDataFrame(data,[\"id\",\"numbers\"])\n","\n","# Define a UDF to calculate the average of a list of numbers\n","def calculate_average(numbers):\n","    return sum(numbers)/len(numbers)\n","\n","average_udf = udf(calculate_average, DoubleType())\n","\n","# Apply the UDF to the dataframe\n","df = df.withColumn(\"average\", average_udf(df[\"numbers\"]))\n","\n","df.show()"]},{"cell_type":"markdown","id":"6aec9fb5","metadata":{"papermill":{"duration":0.061995,"end_time":"2024-01-26T05:19:08.979153","exception":false,"start_time":"2024-01-26T05:19:08.917158","status":"completed"},"tags":[]},"source":["# PySpark UDF to create features"]},{"cell_type":"markdown","id":"1b7fc79e","metadata":{"papermill":{"duration":0.060358,"end_time":"2024-01-26T05:19:09.100842","exception":false,"start_time":"2024-01-26T05:19:09.040484","status":"completed"},"tags":[]},"source":["UDF can be used like apply in pandas dataframes, allowing cumstom logic modifications to columns values.\n","\n","In this example, we will create **a new feature**(new_column) based on the row values of other columns, and use it as features input into our model **LinearRegression**. To do this, we will need to utilise the previously visted **VectorAssemble** as the inputClos."]},{"cell_type":"code","execution_count":35,"id":"0f5e7f87","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:09.224404Z","iopub.status.busy":"2024-01-26T05:19:09.223974Z","iopub.status.idle":"2024-01-26T05:19:11.219721Z","shell.execute_reply":"2024-01-26T05:19:11.217982Z"},"papermill":{"duration":2.061059,"end_time":"2024-01-26T05:19:11.222537","exception":false,"start_time":"2024-01-26T05:19:09.161478","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+---+---+---+----------+\n","| x1| x2|  y|new_column|\n","+---+---+---+----------+\n","|  1|  2|  5|       4.0|\n","|  4|  1| 10|       9.0|\n","|  7|  3| 15|      17.0|\n","+---+---+---+----------+\n","\n","+---+---+---+----------+----------+\n","| x1| x2|  y|new_column|  features|\n","+---+---+---+----------+----------+\n","|  1|  2|  5|       4.0| [1.0,4.0]|\n","|  4|  1| 10|       9.0| [4.0,9.0]|\n","|  7|  3| 15|      17.0|[7.0,17.0]|\n","+---+---+---+----------+----------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import DoubleType\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.regression import LinearRegression\n","\n","# Create a Spark session\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Sample data\n","data=[\n","    (1,2,5),\n","    (4,1,10),\n","    (7,3,15)\n","]\n","df = spark.createDataFrame(data,[\"x1\",\"x2\",\"y\"])\n","\n","# Define a UDF to perform a custom operation\n","def custom_operation(x1,x2):\n","    return x1*2.0 +x2\n","\n","custom_udf = udf(custom_operation, DoubleType())\n","\n","# Apple the UDF to create a new column\n","df= df.withColumn(\"new_column\", custom_udf(df[\"x1\"],df[\"x2\"]))\n","df.show()\n","\n","# Assemble features into a vector\n","assembler = VectorAssembler(inputCols=[\"x1\",\"new_column\"], outputCol=\"features\")\n","df = assembler.transform(df)\n","df.show()"]},{"cell_type":"markdown","id":"e2550ff1","metadata":{"papermill":{"duration":0.061802,"end_time":"2024-01-26T05:19:11.345753","exception":false,"start_time":"2024-01-26T05:19:11.283951","status":"completed"},"tags":[]},"source":["# Pandas UDF"]},{"cell_type":"markdown","id":"e48115d7","metadata":{"papermill":{"duration":0.060177,"end_time":"2024-01-26T05:19:11.467029","exception":false,"start_time":"2024-01-26T05:19:11.406852","status":"completed"},"tags":[]},"source":["**PandasUDFType.SCALAR** is a constant in Pyspark that represents the type of **pandas UDF(SCALAR)**\n","* A scalar pandas UDF takes one or more columns as input and returns a single column as output\n","* It operates on a single rowat a time and can be used to apply arbitary Python functions to the data in a DataFrame"]},{"cell_type":"code","execution_count":36,"id":"865c4237","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:11.591054Z","iopub.status.busy":"2024-01-26T05:19:11.589762Z","iopub.status.idle":"2024-01-26T05:19:12.181293Z","shell.execute_reply":"2024-01-26T05:19:12.180168Z"},"papermill":{"duration":0.6563,"end_time":"2024-01-26T05:19:12.184215","exception":false,"start_time":"2024-01-26T05:19:11.527915","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+---+\n","|   Name|Age|\n","+-------+---+\n","|  alice| 25|\n","|    bob| 30|\n","|charlie| 35|\n","+-------+---+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","# Create a Spark session\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create a sample DataFrame\n","data = [\n","    (\"alice\",25),\n","    (\"bob\",30),\n","    (\"charlie\",35)\n","]\n","\n","df = spark.createDataFrame(data,[\"Name\",\"Age\"])\n","df.show()"]},{"cell_type":"code","execution_count":37,"id":"6a0dc3c8","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:12.310751Z","iopub.status.busy":"2024-01-26T05:19:12.310354Z","iopub.status.idle":"2024-01-26T05:19:15.593591Z","shell.execute_reply":"2024-01-26T05:19:15.592636Z"},"papermill":{"duration":3.350056,"end_time":"2024-01-26T05:19:15.596293","exception":false,"start_time":"2024-01-26T05:19:12.246237","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/pyspark/sql/pandas/functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n","  warnings.warn(\n","[Stage 9:=======================================>                   (2 + 1) / 3]\r"]},{"name":"stdout","output_type":"stream","text":["+-------+---+-----------+\n","|   Name|Age|Capitalised|\n","+-------+---+-----------+\n","|  alice| 25|      Alice|\n","|    bob| 30|        Bob|\n","|charlie| 35|    Charlie|\n","+-------+---+-----------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["from pyspark.sql.functions import pandas_udf, PandasUDFType\n","\n","# Define a pandas UDF\n","# return type & function type\n","\n","@pandas_udf(returnType=\"string\",functionType=PandasUDFType.SCALAR)\n","def capitalise_name(name):\n","    return name.str.capitalize()\n","\n","# Add new column Applythe pandas UDF on the DataFrame\n","df = df.withColumn(\"Capitalised\", capitalise_name(df[\"Name\"]))\n","\n","# Show the result\n","df.show()\n","spark.stop()"]},{"cell_type":"markdown","id":"e4e92d19","metadata":{"papermill":{"duration":0.060756,"end_time":"2024-01-26T05:19:15.719129","exception":false,"start_time":"2024-01-26T05:19:15.658373","status":"completed"},"tags":[]},"source":["Another example, but with more input arguments"]},{"cell_type":"code","execution_count":38,"id":"64838e21","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:15.846356Z","iopub.status.busy":"2024-01-26T05:19:15.845874Z","iopub.status.idle":"2024-01-26T05:19:17.85672Z","shell.execute_reply":"2024-01-26T05:19:17.85537Z"},"papermill":{"duration":2.077854,"end_time":"2024-01-26T05:19:17.860251","exception":false,"start_time":"2024-01-26T05:19:15.782397","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------+------+------+\n","|   name|weight|height|\n","+-------+------+------+\n","|  Alice|    60|   1.6|\n","|    Bob|    75|  1.75|\n","|Charlie|    90|   1.8|\n","+-------+------+------+\n","\n"]}],"source":["from pyspark.sql.types import DoubleType\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Sample data\n","data = [\n","    (\"Alice\",60,1.6),\n","    (\"Bob\",75,1.75),\n","    (\"Charlie\",90,1.8)\n","]\n","df = spark.createDataFrame(data,[\"name\",\"weight\",\"height\"])\n","df.show()\n"]},{"cell_type":"code","execution_count":39,"id":"3ca800dc","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:17.991652Z","iopub.status.busy":"2024-01-26T05:19:17.991203Z","iopub.status.idle":"2024-01-26T05:19:22.268175Z","shell.execute_reply":"2024-01-26T05:19:22.267208Z"},"papermill":{"duration":4.343831,"end_time":"2024-01-26T05:19:22.270882","exception":false,"start_time":"2024-01-26T05:19:17.927051","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------+------+------+------------------+\n","|   name|weight|height|               bmi|\n","+-------+------+------+------------------+\n","|  Alice|    60|   1.6|23.437499999999996|\n","|    Bob|    75|  1.75|24.489795918367346|\n","|Charlie|    90|   1.8|27.777777777777775|\n","+-------+------+------+------------------+\n","\n"]}],"source":["# Define a scalar Pandas UDF to calculate the BMI using weight and height\n","@pandas_udf(DoubleType(), PandasUDFType.SCALAR)\n","def calculate_bmi_udf(weight,height):\n","    return weight / (height**2)\n","\n","# Apply the UDF to the DataFrame\n","df.withColumn(\"bmi\", calculate_bmi_udf(df[\"weight\"],df[\"height\"])).show()\n","spark.stop()"]},{"cell_type":"markdown","id":"4f1b41c4","metadata":{"papermill":{"duration":0.062529,"end_time":"2024-01-26T05:19:22.395885","exception":false,"start_time":"2024-01-26T05:19:22.333356","status":"completed"},"tags":[]},"source":["# Pandas UDF (GROUPED_AGG)"]},{"cell_type":"markdown","id":"dce58fea","metadata":{"papermill":{"duration":0.062224,"end_time":"2024-01-26T05:19:22.520607","exception":false,"start_time":"2024-01-26T05:19:22.458383","status":"completed"},"tags":[]},"source":["**PandasUDFType.GROUPED_AGG** is also a constant in PySpark that represents the typeof a Pandas user-defined function (UDF) for **grouped aggregation**, so it should be used with **groupBy** and **agg**"]},{"cell_type":"code","execution_count":40,"id":"04ee55dd","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:22.646527Z","iopub.status.busy":"2024-01-26T05:19:22.646123Z","iopub.status.idle":"2024-01-26T05:19:24.776306Z","shell.execute_reply":"2024-01-26T05:19:24.775151Z"},"papermill":{"duration":2.197177,"end_time":"2024-01-26T05:19:24.779757","exception":false,"start_time":"2024-01-26T05:19:22.58258","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------+-----+---+\n","|   name|group|age|\n","+-------+-----+---+\n","|  Alice|    A| 34|\n","|    Bob|    A| 28|\n","|Charlie|    B| 45|\n","|  David|    B| 50|\n","+-------+-----+---+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import pandas_udf, PandasUDFType\n","from pyspark.sql.types import DoubleType\n","\n","# Create a Spark session\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Sample data\n","data = [\n","    (\"Alice\",\"A\",34),\n","    (\"Bob\",\"A\",28),\n","    (\"Charlie\",\"B\",45),\n","    (\"David\",\"B\",50)\n","]\n","df = spark.createDataFrame(data,[\"name\",\"group\",\"age\"])\n","df.show()"]},{"cell_type":"code","execution_count":41,"id":"0ef2a774","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:24.929368Z","iopub.status.busy":"2024-01-26T05:19:24.928967Z","iopub.status.idle":"2024-01-26T05:19:28.80958Z","shell.execute_reply":"2024-01-26T05:19:28.808692Z"},"papermill":{"duration":3.9479,"end_time":"2024-01-26T05:19:28.81239","exception":false,"start_time":"2024-01-26T05:19:24.86449","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-----+--------+\n","|group|mean_age|\n","+-----+--------+\n","|    A|    31.0|\n","|    B|    47.5|\n","+-----+--------+\n","\n"]}],"source":["# calculate the men age df['age'] for each group df['group']\n","# Define a grouped aggregate Pandas UDF\n","@pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)\n","def mean_age_udf(age):\n","    return age.mean()\n","\n","# Apply the UDF to the DataFrame\n","\n","result_df = df.groupby(\"group\").agg(mean_age_udf(df[\"age\"]).alias(\"mean_age\"))\n","result_df.show()\n","spark.stop()"]},{"cell_type":"markdown","id":"5577dd9d","metadata":{"papermill":{"duration":0.063205,"end_time":"2024-01-26T05:19:28.93984","exception":false,"start_time":"2024-01-26T05:19:28.876635","status":"completed"},"tags":[]},"source":["# Pandas UDF (GROUPED_MAP)"]},{"cell_type":"code","execution_count":42,"id":"5ca9cd86","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:29.072386Z","iopub.status.busy":"2024-01-26T05:19:29.071746Z","iopub.status.idle":"2024-01-26T05:19:31.122369Z","shell.execute_reply":"2024-01-26T05:19:31.120882Z"},"papermill":{"duration":2.120375,"end_time":"2024-01-26T05:19:31.126737","exception":false,"start_time":"2024-01-26T05:19:29.006362","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+--------+-----+\n","| Product|Sales|\n","+--------+-----+\n","|ProductA|  100|\n","|ProductB|  200|\n","|ProductA|  150|\n","|ProductB|  300|\n","+--------+-----+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import pandas_udf, PandasUDFType\n","import pandas as pd\n","\n","# Create a Spark session\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Sample data\n","data = [\n","    (\"ProductA\",100),\n","    (\"ProductB\",200),\n","    (\"ProductA\",150),\n","    (\"ProductB\",300)\n","]\n","df = spark.createDataFrame(data,[\"Product\",\"Sales\"])\n","df.show()"]},{"cell_type":"code","execution_count":43,"id":"b5287483","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:31.270591Z","iopub.status.busy":"2024-01-26T05:19:31.270152Z","iopub.status.idle":"2024-01-26T05:19:35.186934Z","shell.execute_reply":"2024-01-26T05:19:35.185922Z"},"papermill":{"duration":3.986125,"end_time":"2024-01-26T05:19:35.189724","exception":false,"start_time":"2024-01-26T05:19:31.203599","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/pyspark/sql/pandas/group_ops.py:104: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n","  warnings.warn(\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+--------+-----------+\n","|category|total_sales|\n","+--------+-----------+\n","|ProductA|      250.0|\n","|ProductB|      500.0|\n","+--------+-----------+\n","\n"]}],"source":["# Define a Pandas UDF with PandasUDFType.GROUP_MAP\n","@pandas_udf(\"category string, total_sales double\", PandasUDFType.GROUPED_MAP)\n","def calculate_total_sales(pdf):\n","    category = pdf[\"Product\"].iloc[0] # Get the category from the frst row\n","    total_sales = pdf[\"Sales\"].sum() # Calculate the total sales for the category\n","    return pd.DataFrame({\n","        \"category\":[category],\n","        \"total_sales\":[total_sales]\n","    })\n","\n","# Apply the Pandas UDF to the DataFrame\n","result = df.groupby(\"Product\").apply(calculate_total_sales)\n","result.show()\n","spark.stop()"]},{"cell_type":"markdown","id":"2a0b07a6","metadata":{"papermill":{"duration":0.065286,"end_time":"2024-01-26T05:19:35.322445","exception":false,"start_time":"2024-01-26T05:19:35.257159","status":"completed"},"tags":[]},"source":["Some things to note:\n","While both **GROUP_MAP** and **GROUP_AGG** are used for **grouped operations**(via GroupBy), they serve different purposes:\n","\n","* GROUPED_MAP is used for **applying custom transformations to each group**\n","* GROUPED_AGG is used for **performing aggregate operations on each group**"]},{"cell_type":"code","execution_count":44,"id":"cd90b368","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:35.454779Z","iopub.status.busy":"2024-01-26T05:19:35.454366Z","iopub.status.idle":"2024-01-26T05:19:37.549196Z","shell.execute_reply":"2024-01-26T05:19:37.547744Z"},"papermill":{"duration":2.165357,"end_time":"2024-01-26T05:19:37.552438","exception":false,"start_time":"2024-01-26T05:19:35.387081","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+----------------+----------+-------+\n","|product_category|order_date|revenue|\n","+----------------+----------+-------+\n","|     Electronics|2021-01-01|   1000|\n","|     Electronics|2021-02-01|   1500|\n","|        Clothing|2021-01-01|    800|\n","|        Clothing|2021-02-01|   1200|\n","|        Clothing|2021-03-01|   1500|\n","+----------------+----------+-------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import pandas_udf, PandasUDFType\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create a sample DataFrame\n","data = [\n","    (\"Electronics\", \"2021-01-01\", 1000),\n","    (\"Electronics\", \"2021-02-01\", 1500),\n","    (\"Clothing\", \"2021-01-01\", 800),\n","    (\"Clothing\", \"2021-02-01\", 1200),\n","    (\"Clothing\", \"2021-03-01\", 1500)\n","]\n","df = spark.createDataFrame(data,[\"product_category\", \"order_date\", \"revenue\"])\n","df.show()"]},{"cell_type":"code","execution_count":45,"id":"71d92e78","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:37.684473Z","iopub.status.busy":"2024-01-26T05:19:37.684078Z","iopub.status.idle":"2024-01-26T05:19:41.601117Z","shell.execute_reply":"2024-01-26T05:19:41.600139Z"},"papermill":{"duration":3.9859,"end_time":"2024-01-26T05:19:41.603935","exception":false,"start_time":"2024-01-26T05:19:37.618035","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+------------------+\n","|   average_revenue|\n","+------------------+\n","|1166.6666666666667|\n","|            1250.0|\n","+------------------+\n","\n"]}],"source":["# Define a Python function that takes a pandas DataFrame as input\n","def average_revenue(df):\n","    avg_revenue = df[\"revenue\"].mean()\n","    return pd.DataFrame({\"average_revenue\":[avg_revenue]})\n","\n","# Define a GROUP_MAP pandas UDF\n","grouped_map_udf = pandas_udf(average_revenue, returnType=\"average_revenue double\", functionType=PandasUDFType.GROUPED_MAP)\n","\n","# Apply the GROUP_MAP UDF to the DataFrame grouped by product_category\n","result = df.groupby(\"product_category\").apply(grouped_map_udf)\n","\n","result.show()\n","spark.stop()"]},{"cell_type":"markdown","id":"494856d9","metadata":{"papermill":{"duration":0.064211,"end_time":"2024-01-26T05:19:41.733783","exception":false,"start_time":"2024-01-26T05:19:41.669572","status":"completed"},"tags":[]},"source":["# Non standard Date Parsing"]},{"cell_type":"markdown","id":"5a78d06d","metadata":{"papermill":{"duration":0.064054,"end_time":"2024-01-26T05:19:41.863008","exception":false,"start_time":"2024-01-26T05:19:41.798954","status":"completed"},"tags":[]},"source":["Sometimes we might have date data that doesn't quite fit the required format when reading data into a pyspark dataframe\n","\n","* For example(01.01.2021), in such cases we can do with the utilisation of the **to_date** functions from pyspark.sql.functions,\n","* By default they would need to be read as a **StringFormat** and adjusted after they have been parsed."]},{"cell_type":"code","execution_count":46,"id":"05da70e7","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:41.995061Z","iopub.status.busy":"2024-01-26T05:19:41.9946Z","iopub.status.idle":"2024-01-26T05:19:44.247882Z","shell.execute_reply":"2024-01-26T05:19:44.24636Z"},"papermill":{"duration":2.324484,"end_time":"2024-01-26T05:19:44.252009","exception":false,"start_time":"2024-01-26T05:19:41.927525","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+----------------+----------+-------+\n","|product_category|order_date|revenue|\n","+----------------+----------+-------+\n","|     Electronics|2021.01.01|   1000|\n","|     Electronics|2021.02.01|   1500|\n","|        Clothing|2021.01.01|    800|\n","|        Clothing|2021.02.01|   1200|\n","|        Clothing|2021.03.01|   1500|\n","+----------------+----------+-------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","import pyspark.sql.functions as f\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create a sample DataFrame\n","data = [\n","    (\"Electronics\", \"2021.01.01\", 1000),\n","    (\"Electronics\", \"2021.02.01\", 1500),\n","    (\"Clothing\", \"2021.01.01\", 800),\n","    (\"Clothing\", \"2021.02.01\", 1200),\n","    (\"Clothing\", \"2021.03.01\", 1500)\n","]\n","df = spark.createDataFrame(data,[\"product_category\", \"order_date\", \"revenue\"])\n","df.show()"]},{"cell_type":"code","execution_count":47,"id":"9f20d620","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:44.391728Z","iopub.status.busy":"2024-01-26T05:19:44.391324Z","iopub.status.idle":"2024-01-26T05:19:44.40044Z","shell.execute_reply":"2024-01-26T05:19:44.399234Z"},"papermill":{"duration":0.079575,"end_time":"2024-01-26T05:19:44.402952","exception":false,"start_time":"2024-01-26T05:19:44.323377","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["DataFrame[product_category: string, order_date: string, revenue: bigint]"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":48,"id":"c590cbec","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:44.53569Z","iopub.status.busy":"2024-01-26T05:19:44.535288Z","iopub.status.idle":"2024-01-26T05:19:44.557587Z","shell.execute_reply":"2024-01-26T05:19:44.556415Z"},"papermill":{"duration":0.091337,"end_time":"2024-01-26T05:19:44.560087","exception":false,"start_time":"2024-01-26T05:19:44.46875","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["DataFrame[product_category: string, order_date: string, revenue: bigint, updated_date: date]"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["df.withColumn('updated_date',f.to_date(f.col('order_date'),'yyyy.MM.dd'))"]},{"cell_type":"code","execution_count":49,"id":"0acdf227","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:44.693703Z","iopub.status.busy":"2024-01-26T05:19:44.692959Z","iopub.status.idle":"2024-01-26T05:19:46.303582Z","shell.execute_reply":"2024-01-26T05:19:46.302595Z"},"papermill":{"duration":1.681283,"end_time":"2024-01-26T05:19:46.306921","exception":false,"start_time":"2024-01-26T05:19:44.625638","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------+----------+-------+------------+\n","|product_category|order_date|revenue|updated_date|\n","+----------------+----------+-------+------------+\n","|     Electronics|2021.01.01|   1000|  2021-01-01|\n","|     Electronics|2021.02.01|   1500|  2021-02-01|\n","|        Clothing|2021.01.01|    800|  2021-01-01|\n","|        Clothing|2021.02.01|   1200|  2021-02-01|\n","|        Clothing|2021.03.01|   1500|  2021-03-01|\n","+----------------+----------+-------+------------+\n","\n"]}],"source":["df.withColumn('updated_date',f.to_date(f.col('order_date'),'yyyy.MM.dd')).show()\n","spark.stop()"]},{"cell_type":"markdown","id":"99fc22a1","metadata":{"papermill":{"duration":0.065044,"end_time":"2024-01-26T05:19:46.441866","exception":false,"start_time":"2024-01-26T05:19:46.376822","status":"completed"},"tags":[]},"source":["# Non Standard TimeStamp Parsing"]},{"cell_type":"markdown","id":"ee64dc89","metadata":{"papermill":{"duration":0.066747,"end_time":"2024-01-26T05:19:46.575701","exception":false,"start_time":"2024-01-26T05:19:46.508954","status":"completed"},"tags":[]},"source":["Similar to date parsing, we also have situations when our timestamps don't match the correct format in pyspark(when reading **CSV** files for example)"]},{"cell_type":"code","execution_count":50,"id":"a77a6f3d","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:46.70898Z","iopub.status.busy":"2024-01-26T05:19:46.708512Z","iopub.status.idle":"2024-01-26T05:19:46.849561Z","shell.execute_reply":"2024-01-26T05:19:46.848415Z"},"papermill":{"duration":0.209968,"end_time":"2024-01-26T05:19:46.852064","exception":false,"start_time":"2024-01-26T05:19:46.642096","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["DataFrame[product_category: string, order_time: string, revenue: bigint]"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import to_timestamp\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create a sample DataFrame\n","data = [\n","    (\"Electronics\", \"2021.01.01 10:30:00\", 1000),\n","    (\"Electronics\", \"2021.02.01 14:30:00\", 1500),\n","    (\"Clothing\", \"2021.01.01 09:30:00\", 800),\n","    (\"Clothing\", \"2021.02.01 10:30:00\", 1200),\n","    (\"Clothing\", \"2021.03.01 11:30:00\", 1500)\n","]\n","df = spark.createDataFrame(data, [\"product_category\", \"order_time\", \"revenue\"])\n","df"]},{"cell_type":"code","execution_count":51,"id":"9f77a89e","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:46.986496Z","iopub.status.busy":"2024-01-26T05:19:46.985404Z","iopub.status.idle":"2024-01-26T05:19:47.008465Z","shell.execute_reply":"2024-01-26T05:19:47.007321Z"},"papermill":{"duration":0.09303,"end_time":"2024-01-26T05:19:47.011207","exception":false,"start_time":"2024-01-26T05:19:46.918177","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["DataFrame[product_category: string, order_time: string, revenue: bigint, updated_time: timestamp]"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["df.withColumn('updated_time', to_timestamp(f.col('order_time'),'yyyy.MM.dd HH:mm:ss'))"]},{"cell_type":"code","execution_count":52,"id":"b4adb289","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:47.146781Z","iopub.status.busy":"2024-01-26T05:19:47.146365Z","iopub.status.idle":"2024-01-26T05:19:47.160255Z","shell.execute_reply":"2024-01-26T05:19:47.158863Z"},"papermill":{"duration":0.084478,"end_time":"2024-01-26T05:19:47.16293","exception":false,"start_time":"2024-01-26T05:19:47.078452","status":"completed"},"tags":[]},"outputs":[],"source":["df = df.withColumn('updated_time', to_timestamp(f.col('order_time'),'yyyy.MM.dd HH:mm:ss'))"]},{"cell_type":"code","execution_count":53,"id":"be8b3b9e","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:47.297821Z","iopub.status.busy":"2024-01-26T05:19:47.297378Z","iopub.status.idle":"2024-01-26T05:19:49.22372Z","shell.execute_reply":"2024-01-26T05:19:49.222451Z"},"papermill":{"duration":1.997597,"end_time":"2024-01-26T05:19:49.227057","exception":false,"start_time":"2024-01-26T05:19:47.22946","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+----------------+-------------------+-------+-------------------+\n","|product_category|         order_time|revenue|       updated_time|\n","+----------------+-------------------+-------+-------------------+\n","|     Electronics|2021.01.01 10:30:00|   1000|2021-01-01 10:30:00|\n","|     Electronics|2021.02.01 14:30:00|   1500|2021-02-01 14:30:00|\n","|        Clothing|2021.01.01 09:30:00|    800|2021-01-01 09:30:00|\n","|        Clothing|2021.02.01 10:30:00|   1200|2021-02-01 10:30:00|\n","|        Clothing|2021.03.01 11:30:00|   1500|2021-03-01 11:30:00|\n","+----------------+-------------------+-------+-------------------+\n","\n"]}],"source":["df.show()"]},{"cell_type":"markdown","id":"c17884a4","metadata":{"papermill":{"duration":0.065908,"end_time":"2024-01-26T05:19:49.364971","exception":false,"start_time":"2024-01-26T05:19:49.299063","status":"completed"},"tags":[]},"source":["# Selecing a subset of datetime"]},{"cell_type":"markdown","id":"63bb867e","metadata":{"papermill":{"duration":0.066861,"end_time":"2024-01-26T05:19:49.497369","exception":false,"start_time":"2024-01-26T05:19:49.430508","status":"completed"},"tags":[]},"source":["* When we want to select a particular period, for example for a **particular month**, we can use the **f.month** function from pyspark.sql.functions\n","* To select data with a given condition, we of course need to use **.where()**"]},{"cell_type":"code","execution_count":54,"id":"418bc70b","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:49.630311Z","iopub.status.busy":"2024-01-26T05:19:49.629922Z","iopub.status.idle":"2024-01-26T05:19:49.665757Z","shell.execute_reply":"2024-01-26T05:19:49.664792Z"},"papermill":{"duration":0.105134,"end_time":"2024-01-26T05:19:49.668162","exception":false,"start_time":"2024-01-26T05:19:49.563028","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["DataFrame[product_category: string, order_time: string, revenue: bigint]"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import to_timestamp\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create a sample DataFrame\n","data = [\n","    (\"Electronics\", \"2021-01-01 10:30:00\", 1000),\n","    (\"Electronics\", \"2021-02-01 14:30:00\", 1500),\n","    (\"Electronics\", \"2021-03-01 15:30:00\", 2000),\n","    (\"Clothing\", \"2021-01-01 09:30:00\", 800),\n","    (\"Clothing\", \"2021-02-01 10:30:00\", 1200),\n","    (\"Clothing\", \"2021-03-01 11:30:00\", 1500)\n","]\n","df = spark.createDataFrame(data, [\"product_category\", \"order_time\", \"revenue\"])\n","df"]},{"cell_type":"code","execution_count":55,"id":"e12a7822","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:49.802785Z","iopub.status.busy":"2024-01-26T05:19:49.802328Z","iopub.status.idle":"2024-01-26T05:19:51.001613Z","shell.execute_reply":"2024-01-26T05:19:51.000412Z"},"papermill":{"duration":1.27083,"end_time":"2024-01-26T05:19:51.005413","exception":false,"start_time":"2024-01-26T05:19:49.734583","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------+-------------------+-------+\n","|product_category|         order_time|revenue|\n","+----------------+-------------------+-------+\n","|     Electronics|2021-02-01 14:30:00|   1500|\n","|        Clothing|2021-02-01 10:30:00|   1200|\n","+----------------+-------------------+-------+\n","\n","+----------------+-------------------+-------+\n","|product_category|         order_time|revenue|\n","+----------------+-------------------+-------+\n","|     Electronics|2021-01-01 10:30:00|   1000|\n","|        Clothing|2021-01-01 09:30:00|    800|\n","+----------------+-------------------+-------+\n","\n"]}],"source":["df.where(f.month(f.col('order_time'))==2).show()\n","df.where(f.month(f.col('order_time'))==1).show()"]},{"cell_type":"markdown","id":"10dbd4f8","metadata":{"papermill":{"duration":0.066167,"end_time":"2024-01-26T05:19:51.147818","exception":false,"start_time":"2024-01-26T05:19:51.081651","status":"completed"},"tags":[]},"source":["* We can of course select data for a **given period**, like you would in SQL **BETWEEN** and **AND**\n","* In pyspark however we need to couple **f.col** with **.between(start,finish)** and select the given timeframe, there is no such function **between**"]},{"cell_type":"code","execution_count":56,"id":"9860911b","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:51.281818Z","iopub.status.busy":"2024-01-26T05:19:51.281409Z","iopub.status.idle":"2024-01-26T05:19:51.895054Z","shell.execute_reply":"2024-01-26T05:19:51.893835Z"},"papermill":{"duration":0.683955,"end_time":"2024-01-26T05:19:51.897811","exception":false,"start_time":"2024-01-26T05:19:51.213856","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------+-------------------+-------+\n","|product_category|         order_time|revenue|\n","+----------------+-------------------+-------+\n","|     Electronics|2021-01-01 10:30:00|   1000|\n","|     Electronics|2021-02-01 14:30:00|   1500|\n","|        Clothing|2021-01-01 09:30:00|    800|\n","|        Clothing|2021-02-01 10:30:00|   1200|\n","+----------------+-------------------+-------+\n","\n"]}],"source":["df.where(f.col(\"order_time\").between('2021-01-01','2021-02-05')).show()"]},{"cell_type":"markdown","id":"04c4127c","metadata":{"papermill":{"duration":0.066387,"end_time":"2024-01-26T05:19:52.031077","exception":false,"start_time":"2024-01-26T05:19:51.96469","status":"completed"},"tags":[]},"source":["# Calculating MAU metric"]},{"cell_type":"markdown","id":"e8c4a9f3","metadata":{"papermill":{"duration":0.065006,"end_time":"2024-01-26T05:19:52.162213","exception":false,"start_time":"2024-01-26T05:19:52.097207","status":"completed"},"tags":[]},"source":["* Let's calculate the **Monthly** Active Users(**MAU**) metric\n","* We need to select the data for a particular month & calculate all the unique users (user_id) for this period"]},{"cell_type":"code","execution_count":57,"id":"51ab01d6","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:52.29558Z","iopub.status.busy":"2024-01-26T05:19:52.294864Z","iopub.status.idle":"2024-01-26T05:19:53.005609Z","shell.execute_reply":"2024-01-26T05:19:53.00397Z"},"papermill":{"duration":0.781968,"end_time":"2024-01-26T05:19:53.00952","exception":false,"start_time":"2024-01-26T05:19:52.227552","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-------------+\n","|user_id|activity_date|\n","+-------+-------------+\n","|      1|   2022-01-01|\n","|      2|   2022-01-02|\n","|      3|   2022-01-03|\n","|      1|   2022-02-01|\n","|      2|   2022-02-02|\n","|      3|   2022-02-03|\n","|      4|   2022-02-04|\n","+-------+-------------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql import DataFrame\n","from pyspark.sql.functions import col, countDistinct\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create a sample DataFrame with user_cativity data\n","data = [\n","    (1, '2022-01-01'),\n","    (2, '2022-01-02'),\n","    (3, '2022-01-03'),\n","    (1, '2022-02-01'),\n","    (2, '2022-02-02'),\n","    (3, '2022-02-03'),\n","    (4, '2022-02-04')\n","]\n","df = spark.createDataFrame(data, ['user_id', 'activity_date'])\n","df = df.withColumn('activity_date',f.col('activity_date').cast('date')) # change column type to date from string\n","\n","df.createOrReplaceTempView('user_activity'); df.show()"]},{"cell_type":"code","execution_count":58,"id":"955148d4","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:53.16501Z","iopub.status.busy":"2024-01-26T05:19:53.164599Z","iopub.status.idle":"2024-01-26T05:19:54.115338Z","shell.execute_reply":"2024-01-26T05:19:54.113979Z"},"papermill":{"duration":1.021157,"end_time":"2024-01-26T05:19:54.118461","exception":false,"start_time":"2024-01-26T05:19:53.097304","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 10:===========================================>              (3 + 1) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["+-----------------------+\n","|count(DISTINCT user_id)|\n","+-----------------------+\n","|                      4|\n","+-----------------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["query = \"\"\"\n","SELECT COUNT(distinct(user_id))\n","FROM user_activity\n","WHERE activity_date >= '2022-02-01' AND activity_date < '2022-03-01'\n","\"\"\"\n","\n","spark.sql(query).show()"]},{"cell_type":"markdown","id":"ee896a7b","metadata":{"papermill":{"duration":0.071212,"end_time":"2024-01-26T05:19:54.291334","exception":false,"start_time":"2024-01-26T05:19:54.220122","status":"completed"},"tags":[]},"source":["# Calculating DAU metric"]},{"cell_type":"markdown","id":"6836e5de","metadata":{"papermill":{"duration":0.067444,"end_time":"2024-01-26T05:19:54.426068","exception":false,"start_time":"2024-01-26T05:19:54.358624","status":"completed"},"tags":[]},"source":["* Let's calculate the the **Daily** Active Users (DAU) metric\n","* We need to select the data for a desired period & calculate all the unique users (user_id) for this period using the **groupBy** operation"]},{"cell_type":"code","execution_count":59,"id":"a8f1d137","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:54.578547Z","iopub.status.busy":"2024-01-26T05:19:54.578083Z","iopub.status.idle":"2024-01-26T05:19:55.219082Z","shell.execute_reply":"2024-01-26T05:19:55.217982Z"},"papermill":{"duration":0.714591,"end_time":"2024-01-26T05:19:55.222564","exception":false,"start_time":"2024-01-26T05:19:54.507973","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+-----+-------+\n","|      date| user|service|\n","+----------+-----+-------+\n","|2021-01-01|user1|      1|\n","|2021-01-01|user2|      2|\n","|2021-01-02|user1|      3|\n","|2021-01-02|user3|      4|\n","|2021-01-02|user1|      5|\n","+----------+-----+-------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, count, countDistinct\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create a DataFrame with user activity data\n","data = [\n","    (\"2021-01-01\", \"user1\", '1'),\n","    (\"2021-01-01\", \"user2\", '2'),\n","    (\"2021-01-02\", \"user1\", '3'),\n","    (\"2021-01-02\", \"user3\", '4'),\n","    (\"2021-01-02\", \"user1\", '5'),\n","]\n","df = spark.createDataFrame(data,[\"date\",\"user\",\"service\"])\n","df.createOrReplaceTempView('users')\n","df.show()"]},{"cell_type":"code","execution_count":60,"id":"49f7beb2","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:55.357448Z","iopub.status.busy":"2024-01-26T05:19:55.357055Z","iopub.status.idle":"2024-01-26T05:19:56.976092Z","shell.execute_reply":"2024-01-26T05:19:56.974968Z"},"papermill":{"duration":1.694213,"end_time":"2024-01-26T05:19:56.983845","exception":false,"start_time":"2024-01-26T05:19:55.289632","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+---+\n","|      date|DAU|\n","+----------+---+\n","|2021-01-02|  2|\n","|2021-01-01|  2|\n","+----------+---+\n","\n","+----------+---+\n","|      date| DU|\n","+----------+---+\n","|2021-01-01|  2|\n","|2021-01-02|  3|\n","+----------+---+\n","\n"]}],"source":["# difference b/w count and distinct count\n","df.groupBy(col(\"date\")).agg(countDistinct(col(\"user\")).alias(\"DAU\")).show()  # show unique daily users \n","df.groupBy(f.col('date')).agg(count(col('user')).alias('DU')).show()   # show daily use"]},{"cell_type":"code","execution_count":61,"id":"1887d197","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:57.153611Z","iopub.status.busy":"2024-01-26T05:19:57.153218Z","iopub.status.idle":"2024-01-26T05:19:57.185152Z","shell.execute_reply":"2024-01-26T05:19:57.184293Z"},"papermill":{"duration":0.103602,"end_time":"2024-01-26T05:19:57.187817","exception":false,"start_time":"2024-01-26T05:19:57.084215","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["DataFrame[date: string, DAU: bigint]"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["df.groupBy(col('date')).agg(countDistinct('user').alias('DAU'))"]},{"cell_type":"code","execution_count":62,"id":"1c716aa8","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:57.364831Z","iopub.status.busy":"2024-01-26T05:19:57.364393Z","iopub.status.idle":"2024-01-26T05:19:58.062129Z","shell.execute_reply":"2024-01-26T05:19:58.060821Z"},"papermill":{"duration":0.791497,"end_time":"2024-01-26T05:19:58.065656","exception":false,"start_time":"2024-01-26T05:19:57.274159","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+\n","|count(DISTINCT user)|\n","+--------------------+\n","|                   2|\n","|                   2|\n","+--------------------+\n","\n"]}],"source":["query = \"\"\"\n","SELECT COUNT(distinct(user))\n","FROM users\n","GROUP BY date\n","\"\"\"\n","\n","spark.sql(query).show()"]},{"cell_type":"markdown","id":"cd9f9900","metadata":{"papermill":{"duration":0.068026,"end_time":"2024-01-26T05:19:58.223251","exception":false,"start_time":"2024-01-26T05:19:58.155225","status":"completed"},"tags":[]},"source":["# Calculating LTV metric"]},{"cell_type":"markdown","id":"457fa5b9","metadata":{"papermill":{"duration":0.067273,"end_time":"2024-01-26T05:19:58.359844","exception":false,"start_time":"2024-01-26T05:19:58.292571","status":"completed"},"tags":[]},"source":["Lifetime Value (LTV)\n","\n","* 1.Calculate the total revenue for each customer.\n","* 2.Calculate the average revenue per customer.\n","* 3.Calculate the average customer lifespan.\n","* 4.Multiply the avrage customer revenue by the average lifespan to get the LTV."]},{"cell_type":"code","execution_count":63,"id":"7513b0d8","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:58.496163Z","iopub.status.busy":"2024-01-26T05:19:58.495741Z","iopub.status.idle":"2024-01-26T05:19:59.088221Z","shell.execute_reply":"2024-01-26T05:19:59.087079Z"},"papermill":{"duration":0.664215,"end_time":"2024-01-26T05:19:59.091491","exception":false,"start_time":"2024-01-26T05:19:58.427276","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+----------+-------+\n","| customer|      date|revenue|\n","+---------+----------+-------+\n","|customer1|2021-01-01|    100|\n","|customer2|2021-01-02|    150|\n","|customer2|2021-01-01|    200|\n","|customer2|2021-01-02|    300|\n","+---------+----------+-------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import sum, avg\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create a DataFrame with customer revenue data\n","data = [\n","    (\"customer1\", \"2021-01-01\", 100),\n","    (\"customer2\", \"2021-01-02\", 150),\n","    (\"customer2\", \"2021-01-01\", 200),\n","    (\"customer2\", \"2021-01-02\", 300)\n","]\n","df = spark.createDataFrame(data, [\"customer\", \"date\", \"revenue\"])\n","df.show()"]},{"cell_type":"markdown","id":"55a525b7","metadata":{"papermill":{"duration":0.067264,"end_time":"2024-01-26T05:19:59.235683","exception":false,"start_time":"2024-01-26T05:19:59.168419","status":"completed"},"tags":[]},"source":["Let's calculate the **total and average** cutomer revenue"]},{"cell_type":"code","execution_count":64,"id":"6928727a","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:19:59.372852Z","iopub.status.busy":"2024-01-26T05:19:59.37242Z","iopub.status.idle":"2024-01-26T05:20:00.091793Z","shell.execute_reply":"2024-01-26T05:20:00.090679Z"},"papermill":{"duration":0.791176,"end_time":"2024-01-26T05:20:00.095108","exception":false,"start_time":"2024-01-26T05:19:59.303932","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+\n","|avg_revenue|\n","+-----------+\n","|      375.0|\n","+-----------+\n","\n"]}],"source":["# total and average revenue per customer\n","total_revenue = df.groupby('customer').agg(f.sum(f.col('revenue')).alias('total_revenue'))\n","\n","avg_revenue = total_revenue.select(f.avg('total_revenue').alias('avg_revenue'))\n","avg_revenue.show()"]},{"cell_type":"markdown","id":"3e60b0f7","metadata":{"papermill":{"duration":0.06831,"end_time":"2024-01-26T05:20:00.241104","exception":false,"start_time":"2024-01-26T05:20:00.172794","status":"completed"},"tags":[]},"source":["Using **dataframe.selectExpr** we can select/calculate the difference between two dates"]},{"cell_type":"code","execution_count":65,"id":"1d2f05c0","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:00.379029Z","iopub.status.busy":"2024-01-26T05:20:00.378584Z","iopub.status.idle":"2024-01-26T05:20:01.149834Z","shell.execute_reply":"2024-01-26T05:20:01.148741Z"},"papermill":{"duration":0.844021,"end_time":"2024-01-26T05:20:01.153028","exception":false,"start_time":"2024-01-26T05:20:00.309007","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+\n","|lifespan|\n","+--------+\n","|       1|\n","+--------+\n","\n"]}],"source":["# Calculate average customer lifespan (in days)\n","lifespan = df.selectExpr(\"DATEDIFF(max(date), min(date)) as lifespan\")\n","lifespan.show()"]},{"cell_type":"code","execution_count":66,"id":"11c93105","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:01.333597Z","iopub.status.busy":"2024-01-26T05:20:01.333215Z","iopub.status.idle":"2024-01-26T05:20:03.736399Z","shell.execute_reply":"2024-01-26T05:20:03.735528Z"},"papermill":{"duration":2.489246,"end_time":"2024-01-26T05:20:03.740273","exception":false,"start_time":"2024-01-26T05:20:01.251027","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-----------+--------+\n","|avg_revenue|lifespan|\n","+-----------+--------+\n","|      375.0|       1|\n","+-----------+--------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 54:=============================>                            (2 + 2) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["+-----+\n","|  LTV|\n","+-----+\n","|375.0|\n","+-----+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Cross Join Both Tables\n","joined = avg_revenue.crossJoin(lifespan)\n","joined.show()\n","joined.selectExpr(\"avg_revenue * lifespan as LTV\").show()"]},{"cell_type":"markdown","id":"c2ad367f","metadata":{"papermill":{"duration":0.086915,"end_time":"2024-01-26T05:20:03.921455","exception":false,"start_time":"2024-01-26T05:20:03.83454","status":"completed"},"tags":[]},"source":["# Calculating ARPU metric"]},{"cell_type":"markdown","id":"a9d6ae21","metadata":{"papermill":{"duration":0.067568,"end_time":"2024-01-26T05:20:04.058514","exception":false,"start_time":"2024-01-26T05:20:03.990946","status":"completed"},"tags":[]},"source":["**Average Revenue Per User(ARPU)** metric using **PySpark**\n","\n","> The Average Revenue Per User (ARPU) is a key performance indicator used in business and financial analysis to measure the average revenue generated by each user or customer. It is commonly used in industries such as telecommunications, software as a service (SaaS), and subscription-based businesses."]},{"cell_type":"code","execution_count":67,"id":"64da201a","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:04.199287Z","iopub.status.busy":"2024-01-26T05:20:04.198876Z","iopub.status.idle":"2024-01-26T05:20:04.844401Z","shell.execute_reply":"2024-01-26T05:20:04.843065Z"},"papermill":{"duration":0.72083,"end_time":"2024-01-26T05:20:04.848738","exception":false,"start_time":"2024-01-26T05:20:04.127908","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+--------+-------+\n","|customer_id|order_id|revenue|\n","+-----------+--------+-------+\n","|          1|     100|   50.0|\n","|          1|     101|   30.0|\n","|          2|     102|   70.0|\n","|          2|     103|   60.0|\n","|          3|     104|   40.0|\n","|          3|     105|   20.0|\n","+-----------+--------+-------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import sum, count, avg\n","\n","# Create a Spark session\n","spark = SparkSession.builder.appName(\"ARPU Calculation\").getOrCreate()\n","\n","# Sample customer transactions data\n","data = [\n","    (1,100,50.0),\n","    (1,101,30.0),\n","    (2,102,70.0),\n","    (2,103,60.0),\n","    (3,104,40.0),\n","    (3,105,20.0)\n","]\n","\n","# Create a DataFrame from the sample data\n","columns = [\"customer_id\", \"order_id\", \"revenue\"]\n","transactions_df = spark.createDataFrame(data, columns)\n","transactions_df.show()"]},{"cell_type":"code","execution_count":68,"id":"22a07e66","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:05.002404Z","iopub.status.busy":"2024-01-26T05:20:05.00188Z","iopub.status.idle":"2024-01-26T05:20:06.999607Z","shell.execute_reply":"2024-01-26T05:20:06.997964Z"},"papermill":{"duration":2.071443,"end_time":"2024-01-26T05:20:07.003351","exception":false,"start_time":"2024-01-26T05:20:04.931908","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+-------------+\n","|customer_id|total_revenue|\n","+-----------+-------------+\n","|          1|         80.0|\n","|          2|        130.0|\n","|          3|         60.0|\n","+-----------+-------------+\n","\n","+----+\n","|arpu|\n","+----+\n","|90.0|\n","+----+\n","\n"]}],"source":["from pyspark.sql.functions import countDistinct\n","\n","# Calculat the total revenue per customer\n","revenue_per_customer = transactions_df.groupBy(\"customer_id\").agg(sum(\"revenue\").alias(\"total_revenue\"))\n","revenue_per_customer.show()\n","\n","# Calculate the total number of unique customers\n","total_customers = transactions_df.select(\"customer_id\").distinct().count()\n","\n","# Calculate the ARPU by dividing the total revenue by the total number of unique customers\n","arpu = revenue_per_customer.agg((sum(\"total_revenue\")/total_customers).alias('arpu'))\n","arpu.show()"]},{"cell_type":"markdown","id":"d5c3792d","metadata":{"papermill":{"duration":0.068713,"end_time":"2024-01-26T05:20:07.150204","exception":false,"start_time":"2024-01-26T05:20:07.081491","status":"completed"},"tags":[]},"source":["# calculating ARPPU meric"]},{"cell_type":"markdown","id":"aaadaa14","metadata":{"papermill":{"duration":0.06889,"end_time":"2024-01-26T05:20:07.288175","exception":false,"start_time":"2024-01-26T05:20:07.219285","status":"completed"},"tags":[]},"source":["**Average Revenue Per Paying User (ARPPU) metric using PySpark**\n","\n","> The Average Revenue Per Paying User (ARPPU) metric is a key performance indicator used in the gaming, subscription-based services, and other industries to measure the average revenue generated from each paying customer. It provides insights into the spending behavior of paying users and help businesses understand the value they derive from each customer who makes a purchase or subscribes to a service."]},{"cell_type":"code","execution_count":69,"id":"79f78e00","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:07.425831Z","iopub.status.busy":"2024-01-26T05:20:07.425452Z","iopub.status.idle":"2024-01-26T05:20:08.028037Z","shell.execute_reply":"2024-01-26T05:20:08.026809Z"},"papermill":{"duration":0.67503,"end_time":"2024-01-26T05:20:08.031156","exception":false,"start_time":"2024-01-26T05:20:07.356126","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+-------+-------+-----------+\n","| id|   name|revenue|paying_user|\n","+---+-------+-------+-----------+\n","|  1|  Alice|    100|       true|\n","|  2|    Bob|    200|       true|\n","|  3|Charlie|    300|      false|\n","|  4|  David|    150|       true|\n","+---+-------+-------+-----------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","# Create a Spark session\n","spark = SparkSession.builder.appName(\"ARPPU Calculation\").getOrCreate()\n","\n","# Sample data\n","data = [\n","    (1, \"Alice\", 100, True),\n","    (2, \"Bob\", 200,True),\n","    (3, \"Charlie\", 300, False),\n","    (4, \"David\", 150, True)\n","]\n","\n","# Create a DataFrame with the sample data\n","df = spark.createDataFrame(data, [\"id\", \"name\", \"revenue\", \"paying_user\"])\n","df.show()"]},{"cell_type":"code","execution_count":70,"id":"d01af56b","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:08.250904Z","iopub.status.busy":"2024-01-26T05:20:08.250453Z","iopub.status.idle":"2024-01-26T05:20:09.766219Z","shell.execute_reply":"2024-01-26T05:20:09.764963Z"},"papermill":{"duration":1.658572,"end_time":"2024-01-26T05:20:09.768847","exception":false,"start_time":"2024-01-26T05:20:08.110275","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------+\n","|total_revenue|\n","+-------------+\n","|          450|\n","+-------------+\n","\n","+-----+\n","|arppu|\n","+-----+\n","|150.0|\n","+-----+\n","\n"]}],"source":["# Calculate the total revenue from paying users\n","paying_counts = df.filter(df.paying_user == True).count()\n","paying_revenue = df.filter(df.paying_user == True).agg(sum(\"revenue\").alias('total_revenue'))\n","paying_revenue.show()\n","\n","paying_revenue.select((f.col('total_revenue')/paying_counts).alias('arppu')).show()"]},{"cell_type":"markdown","id":"cc3d0858","metadata":{"papermill":{"duration":0.068323,"end_time":"2024-01-26T05:20:09.906153","exception":false,"start_time":"2024-01-26T05:20:09.83783","status":"completed"},"tags":[]},"source":["# Calculating ROI metric"]},{"cell_type":"markdown","id":"d5124227","metadata":{"papermill":{"duration":0.068044,"end_time":"2024-01-26T05:20:10.044797","exception":false,"start_time":"2024-01-26T05:20:09.976753","status":"completed"},"tags":[]},"source":["**Return on investment (ROI)** is a financial metric that measures the profititability of an investment"]},{"cell_type":"code","execution_count":71,"id":"2f31e39b","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:10.184505Z","iopub.status.busy":"2024-01-26T05:20:10.183801Z","iopub.status.idle":"2024-01-26T05:20:10.799635Z","shell.execute_reply":"2024-01-26T05:20:10.798508Z"},"papermill":{"duration":0.688867,"end_time":"2024-01-26T05:20:10.803158","exception":false,"start_time":"2024-01-26T05:20:10.114291","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+------+------+\n","|  Project|  Cost|Profit|\n","+---------+------+------+\n","|Project A|100000|150000|\n","|Project B| 80000|120000|\n","|Project C|120000| 90000|\n","+---------+------+------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","\n","# Create a Spark session\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create a DataFrame representing the investment data\n","data = [\n","    (\"Project A\", 100000, 150000),  # (Project Name, Cost of Investment, Net Profit)\n","    (\"Project B\", 80000, 120000),\n","    (\"Project C\", 120000, 90000)\n","]\n","df = spark.createDataFrame(data, [\"Project\", \"Cost\", \"Profit\"])\n","df.show()"]},{"cell_type":"code","execution_count":72,"id":"fbbb5af3","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:10.950056Z","iopub.status.busy":"2024-01-26T05:20:10.949659Z","iopub.status.idle":"2024-01-26T05:20:11.524587Z","shell.execute_reply":"2024-01-26T05:20:11.523429Z"},"papermill":{"duration":0.6495,"end_time":"2024-01-26T05:20:11.528204","exception":false,"start_time":"2024-01-26T05:20:10.878704","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+------+------+-----+\n","|  Project|  Cost|Profit|  ROI|\n","+---------+------+------+-----+\n","|Project A|100000|150000| 50.0|\n","|Project B| 80000|120000| 50.0|\n","|Project C|120000| 90000|-25.0|\n","+---------+------+------+-----+\n","\n"]}],"source":["# Calculate ROI using PySpark\n","roi_df = df.withColumn(\"ROI\", (col(\"Profit\")/col(\"Cost\")-1)*100)\n","roi_df.show()"]},{"cell_type":"markdown","id":"9eb7f221","metadata":{"papermill":{"duration":0.068375,"end_time":"2024-01-26T05:20:11.677399","exception":false,"start_time":"2024-01-26T05:20:11.609024","status":"completed"},"tags":[]},"source":["# Calculating the retention rate metric"]},{"cell_type":"markdown","id":"c302a9ae","metadata":{"papermill":{"duration":0.068627,"end_time":"2024-01-26T05:20:11.815194","exception":false,"start_time":"2024-01-26T05:20:11.746567","status":"completed"},"tags":[]},"source":["**Retention Rate:**\n","> Retention rate is a metric used to measure the percentage of customers or users who continue to use a product, service, or platform over a certain period of time. In the context of a business or a subscription-based service, retention rate is a key indicator of customer loyalty and satisfaction."]},{"cell_type":"code","execution_count":73,"id":"ae4bca01","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:11.955375Z","iopub.status.busy":"2024-01-26T05:20:11.95497Z","iopub.status.idle":"2024-01-26T05:20:12.589478Z","shell.execute_reply":"2024-01-26T05:20:12.58832Z"},"papermill":{"duration":0.709402,"end_time":"2024-01-26T05:20:12.593151","exception":false,"start_time":"2024-01-26T05:20:11.883749","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-------+\n","|user_id|  month|\n","+-------+-------+\n","|      1|2020-01|\n","|      2|2020-01|\n","|      3|2020-01|\n","|      1|2020-02|\n","|      3|2020-02|\n","|      4|2020-02|\n","|      1|2020-03|\n","|      4|2020-03|\n","|      5|2020-03|\n","+-------+-------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, countDistinct, lag\n","from pyspark.sql.window import Window\n","\n","# Create a Spark session\n","spark = SparkSession.builder.getOrCreate()\n","spark.sparkContext.setLogLevel(\"ERROR\")\n","\n","# Sample data\n","data = [\n","    (1,\"2020-01\"),\n","    (2,\"2020-01\"),\n","    (3,\"2020-01\"),\n","    (1,\"2020-02\"),\n","    (3,\"2020-02\"),\n","    (4,\"2020-02\"),\n","    (1,\"2020-03\"),\n","    (4,\"2020-03\"),\n","    (5,\"2020-03\")\n","]\n","\n","# Create some user\n","df = spark.createDataFrame(data,[\"user_id\", \"month\"]) # Create a DataFrame from the sample data\n","df.show()"]},{"cell_type":"code","execution_count":74,"id":"462445db","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:12.750493Z","iopub.status.busy":"2024-01-26T05:20:12.750102Z","iopub.status.idle":"2024-01-26T05:20:13.502855Z","shell.execute_reply":"2024-01-26T05:20:13.501985Z"},"papermill":{"duration":0.827087,"end_time":"2024-01-26T05:20:13.506196","exception":false,"start_time":"2024-01-26T05:20:12.679109","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-------------+\n","|  month|monthly_users|\n","+-------+-------------+\n","|2020-02|            3|\n","|2020-03|            3|\n","|2020-01|            3|\n","+-------+-------------+\n","\n"]}],"source":["monthly_users = df.groupBy(\"month\").agg(countDistinct(\"user_id\").alias(\"monthly_users\")) # Group the data by month and count the number of unique users\n","monthly_users.show()"]},{"cell_type":"code","execution_count":75,"id":"6564a09d","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:13.655965Z","iopub.status.busy":"2024-01-26T05:20:13.655538Z","iopub.status.idle":"2024-01-26T05:20:14.386218Z","shell.execute_reply":"2024-01-26T05:20:14.384968Z"},"papermill":{"duration":0.804509,"end_time":"2024-01-26T05:20:14.389508","exception":false,"start_time":"2024-01-26T05:20:13.584999","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-------------+--------------------+\n","|  month|monthly_users|previous_month_users|\n","+-------+-------------+--------------------+\n","|2020-01|            3|                NULL|\n","|2020-02|            3|                   3|\n","|2020-03|            3|                   3|\n","+-------+-------------+--------------------+\n","\n"]}],"source":["# Calculate the number of users retained from the previous month by using lag (like in SQL)\n","windowSpec = Window.orderBy(\"month\") # define the window\n","monthly_users = monthly_users.withColumn(\"previous_month_users\", lag(\"monthly_users\").over(windowSpec))\n","monthly_users.show()"]},{"cell_type":"code","execution_count":76,"id":"3ce53e92","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:14.538273Z","iopub.status.busy":"2024-01-26T05:20:14.537832Z","iopub.status.idle":"2024-01-26T05:20:15.156902Z","shell.execute_reply":"2024-01-26T05:20:15.155712Z"},"papermill":{"duration":0.693309,"end_time":"2024-01-26T05:20:15.160494","exception":false,"start_time":"2024-01-26T05:20:14.467185","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-------------+--------------------+--------------+\n","|  month|monthly_users|previous_month_users|retention_rate|\n","+-------+-------------+--------------------+--------------+\n","|2020-01|            3|                NULL|          NULL|\n","|2020-02|            3|                   3|         100.0|\n","|2020-03|            3|                   3|         100.0|\n","+-------+-------------+--------------------+--------------+\n","\n"]}],"source":["# Calculate the retention rate (all users)\n","monthly_users = monthly_users.withColumn(\"retention_rate\", (monthly_users[\"monthly_users\"]/monthly_users[\"previous_month_users\"])*100)\n","monthly_users.show()"]},{"cell_type":"markdown","id":"7202de6d","metadata":{"papermill":{"duration":0.082094,"end_time":"2024-01-26T05:20:15.329327","exception":false,"start_time":"2024-01-26T05:20:15.247233","status":"completed"},"tags":[]},"source":["# Calculating ROMI metric"]},{"cell_type":"markdown","id":"8c42c146","metadata":{"papermill":{"duration":0.069352,"end_time":"2024-01-26T05:20:15.467812","exception":false,"start_time":"2024-01-26T05:20:15.39846","status":"completed"},"tags":[]},"source":["**Return on Marketing Investment (ROMI)**\n","> ROMI (Return on Marketing Investment) is a metric used to measure the effectiveness of marketing campaigns by comparing the revenue generated from the campaign to the cost of the campaign"]},{"cell_type":"code","execution_count":77,"id":"2f7aebcc","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:15.608012Z","iopub.status.busy":"2024-01-26T05:20:15.607622Z","iopub.status.idle":"2024-01-26T05:20:16.211729Z","shell.execute_reply":"2024-01-26T05:20:16.210606Z"},"papermill":{"duration":0.67861,"end_time":"2024-01-26T05:20:16.21536","exception":false,"start_time":"2024-01-26T05:20:15.53675","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+----+-------+----------+\n","|campaign_id|cost|revenue|      date|\n","+-----------+----+-------+----------+\n","|          1|1000|   5000|2021-01-01|\n","|          2|1500|   6000|2021-01-15|\n","|          3|2000|   8000|2021-02-01|\n","+-----------+----+-------+----------+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql import functions as f\n","\n","# Create a Spark session\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Sample marketing data\n","data = [\n","    (1,1000,5000,\"2021-01-01\"),\n","    (2,1500,6000,\"2021-01-15\"),\n","    (3,2000,8000,\"2021-02-01\")\n","]\n","\n","# Create a DataFrame from the sample data\n","marketing_data = spark.createDataFrame(data, [\"campaign_id\",\"cost\", \"revenue\", \"date\"])\n","marketing_data.show()"]},{"cell_type":"code","execution_count":78,"id":"34bac2e6","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:16.364234Z","iopub.status.busy":"2024-01-26T05:20:16.363221Z","iopub.status.idle":"2024-01-26T05:20:16.993058Z","shell.execute_reply":"2024-01-26T05:20:16.991949Z"},"papermill":{"duration":0.702934,"end_time":"2024-01-26T05:20:16.996263","exception":false,"start_time":"2024-01-26T05:20:16.293329","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+----+-------+----------+----+\n","|campaign_id|cost|revenue|      date|romi|\n","+-----------+----+-------+----------+----+\n","|          1|1000|   5000|2021-01-01| 4.0|\n","|          2|1500|   6000|2021-01-15| 3.0|\n","|          3|2000|   8000|2021-02-01| 3.0|\n","+-----------+----+-------+----------+----+\n","\n"]}],"source":["# Calculate ROMI using SQL notation f.expr, like lambda\n","romi_data = marketing_data.withColumn(\"romi\", f.expr(\"(revenue - cost)/cost\"))\n","romi_data.show()"]},{"cell_type":"markdown","id":"b6fa305e","metadata":{"papermill":{"duration":0.069604,"end_time":"2024-01-26T05:20:17.151117","exception":false,"start_time":"2024-01-26T05:20:17.081513","status":"completed"},"tags":[]},"source":["# Feature Engineering with Binarizer"]},{"cell_type":"markdown","id":"1180c6d1","metadata":{"papermill":{"duration":0.070209,"end_time":"2024-01-26T05:20:17.291113","exception":false,"start_time":"2024-01-26T05:20:17.220904","status":"completed"},"tags":[]},"source":["**Binarizer** is a Transformer which applies a threshold to a numeric field, turning it into 0s (below threshold) and 1s (above threshold)\n","\n","* > The method iyself is accessable from **pyspark.ml.feature** and requires a **double** input dtype\n","* > To convert data types after the schema has been set or created, use **withColumn w/ col().cast()**"]},{"cell_type":"code","execution_count":79,"id":"f55711f3","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:17.434049Z","iopub.status.busy":"2024-01-26T05:20:17.433615Z","iopub.status.idle":"2024-01-26T05:20:18.185221Z","shell.execute_reply":"2024-01-26T05:20:18.184158Z"},"papermill":{"duration":0.826277,"end_time":"2024-01-26T05:20:18.187571","exception":false,"start_time":"2024-01-26T05:20:17.361294","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+---+------+---------------+-------------+---------------+-----+\n","|customer_id|age|gender|monthly_charges|total_charges|contract_length|churn|\n","+-----------+---+------+---------------+-------------+---------------+-----+\n","|          1| 45|     M|             75|          900|             12|   No|\n","|          2| 30|     F|             60|          720|              6|  Yes|\n","|          3| 50|     M|             85|         1020|             24|   No|\n","|          4| 35|     F|             70|          840|             12|  Yes|\n","|          5| 55|     M|             95|         1140|             24|   No|\n","|          6| 40|     F|             80|          960|              6|   No|\n","|          7| 25|     M|             55|          660|              6|  Yes|\n","|          8| 60|     F|            100|         1200|             12|   No|\n","|          9| 50|     M|             90|         1080|             24|   No|\n","|         10| 35|     F|             65|          780|              6|  Yes|\n","+-----------+---+------+---------------+-------------+---------------+-----+\n","\n"]},{"data":{"text/plain":["DataFrame[customer_id: int, age: int, gender: string, monthly_charges: int, total_charges: int, contract_length: int, churn: string]"]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n","\n","# Create a Spark session\n","spark = SparkSession.builder.getOrCreate()\n","spark.sparkContext.setLogLevel(\"ERROR\")\n","\n","# Define the schema using StructType and StructField\n","schema = StructType([\n","    StructField(\"customer_id\", IntegerType(), True),\n","    StructField(\"age\", IntegerType(), True),\n","    StructField(\"gender\", StringType(), True),\n","    StructField(\"monthly_charges\", IntegerType(), True),\n","    StructField(\"total_charges\", IntegerType(), True),\n","    StructField(\"contract_length\", IntegerType(), True),\n","    StructField(\"churn\", StringType(), True)\n","])\n","\n","# Churn related data\n","data = [\n","    (1,45,'M',75,900,12,'No'),\n","    (2,30,'F',60,720,6,'Yes'),\n","    (3,50,'M',85,1020,24,'No'),\n","    (4,35,'F',70,840,12,'Yes'),\n","    (5,55,'M',95,1140,24,'No'),\n","    (6,40,'F',80,960,6,'No'),\n","    (7,25,'M',55,660,6,'Yes'),\n","    (8,60,'F',100,1200,12,'No'),\n","    (9,50,'M',90,1080,24,'No'),\n","    (10,35,'F',65,780,6,'Yes')\n","]\n","\n","spark = SparkSession.builder.appName(\"churn_analysis\").getOrCreate()\n","df = spark.createDataFrame(data, schema)\n","df.show()\n","df"]},{"cell_type":"code","execution_count":80,"id":"1d805038","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:18.332614Z","iopub.status.busy":"2024-01-26T05:20:18.332185Z","iopub.status.idle":"2024-01-26T05:20:18.931349Z","shell.execute_reply":"2024-01-26T05:20:18.930228Z"},"papermill":{"duration":0.675841,"end_time":"2024-01-26T05:20:18.934818","exception":false,"start_time":"2024-01-26T05:20:18.258977","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+----+------+---------------+-------------+---------------+-----+\n","|customer_id| age|gender|monthly_charges|total_charges|contract_length|churn|\n","+-----------+----+------+---------------+-------------+---------------+-----+\n","|          1|45.0|     M|             75|          900|             12|   No|\n","|          2|30.0|     F|             60|          720|              6|  Yes|\n","|          3|50.0|     M|             85|         1020|             24|   No|\n","|          4|35.0|     F|             70|          840|             12|  Yes|\n","|          5|55.0|     M|             95|         1140|             24|   No|\n","|          6|40.0|     F|             80|          960|              6|   No|\n","|          7|25.0|     M|             55|          660|              6|  Yes|\n","|          8|60.0|     F|            100|         1200|             12|   No|\n","|          9|50.0|     M|             90|         1080|             24|   No|\n","|         10|35.0|     F|             65|          780|              6|  Yes|\n","+-----------+----+------+---------------+-------------+---------------+-----+\n","\n"]}],"source":["from pyspark.ml.feature import Binarizer\n","from pyspark.sql.functions import col, countDistinct, lag\n","\n","df = df.withColumn('age', col('age').cast('double'))\n","df.show()"]},{"cell_type":"code","execution_count":81,"id":"dc43f9bd","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:19.082961Z","iopub.status.busy":"2024-01-26T05:20:19.08229Z","iopub.status.idle":"2024-01-26T05:20:19.868699Z","shell.execute_reply":"2024-01-26T05:20:19.867614Z"},"papermill":{"duration":0.862187,"end_time":"2024-01-26T05:20:19.87154","exception":false,"start_time":"2024-01-26T05:20:19.009353","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+----+------+---------------+-------------+---------------+-----+------------+\n","|customer_id| age|gender|monthly_charges|total_charges|contract_length|churn|age_above_30|\n","+-----------+----+------+---------------+-------------+---------------+-----+------------+\n","|          1|45.0|     M|             75|          900|             12|   No|         1.0|\n","|          2|30.0|     F|             60|          720|              6|  Yes|         0.0|\n","|          3|50.0|     M|             85|         1020|             24|   No|         1.0|\n","|          4|35.0|     F|             70|          840|             12|  Yes|         1.0|\n","|          5|55.0|     M|             95|         1140|             24|   No|         1.0|\n","|          6|40.0|     F|             80|          960|              6|   No|         1.0|\n","|          7|25.0|     M|             55|          660|              6|  Yes|         0.0|\n","|          8|60.0|     F|            100|         1200|             12|   No|         1.0|\n","|          9|50.0|     M|             90|         1080|             24|   No|         1.0|\n","|         10|35.0|     F|             65|          780|              6|  Yes|         1.0|\n","+-----------+----+------+---------------+-------------+---------------+-----+------------+\n","\n"]}],"source":["# add the new binarized format,note that the input format into Binarizer is \"double\"\n","binariser = Binarizer(inputCol='age',outputCol='age_above_30',threshold=30.0)\n","df = binariser.transform(df)\n","df.show()\n"]},{"cell_type":"markdown","id":"f4099937","metadata":{"papermill":{"duration":0.069819,"end_time":"2024-01-26T05:20:20.01209","exception":false,"start_time":"2024-01-26T05:20:19.942271","status":"completed"},"tags":[]},"source":["# Converting Column Types"]},{"cell_type":"markdown","id":"79ddfd51","metadata":{"papermill":{"duration":0.070104,"end_time":"2024-01-26T05:20:20.156796","exception":false,"start_time":"2024-01-26T05:20:20.086692","status":"completed"},"tags":[]},"source":["> * As we saw in the above example, **input type management** is quite important in pyspark\n","> * Let's review the methods we can use to convert column/attribute types, so we don't forget how to do it"]},{"cell_type":"code","execution_count":82,"id":"79133191","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:20.297768Z","iopub.status.busy":"2024-01-26T05:20:20.297365Z","iopub.status.idle":"2024-01-26T05:20:20.316583Z","shell.execute_reply":"2024-01-26T05:20:20.315543Z"},"papermill":{"duration":0.093213,"end_time":"2024-01-26T05:20:20.319462","exception":false,"start_time":"2024-01-26T05:20:20.226249","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["DataFrame[customer_id: int, age: double, gender: string, monthly_charges: int, total_charges: int, contract_length: int, churn: string, age_above_30: double]\n","DataFrame[customer_id: int, age: int, gender: string, monthly_charges: int, total_charges: int, contract_length: int, churn: string, age_above_30: double]\n"]}],"source":["from pyspark.sql.functions import col\n","\n","# Convert a column to a different data type\n","print(df)\n","df = df.withColumn(\"age\", col(\"age\").cast(\"integer\"))\n","print(df)"]},{"cell_type":"code","execution_count":83,"id":"37c3db16","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:20.460799Z","iopub.status.busy":"2024-01-26T05:20:20.460364Z","iopub.status.idle":"2024-01-26T05:20:21.116365Z","shell.execute_reply":"2024-01-26T05:20:21.115298Z"},"papermill":{"duration":0.730481,"end_time":"2024-01-26T05:20:21.119837","exception":false,"start_time":"2024-01-26T05:20:20.389356","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+---+------+---------------+-------------+---------------+-----+------------+---------+\n","|customer_id|age|gender|monthly_charges|total_charges|contract_length|churn|age_above_30|age_float|\n","+-----------+---+------+---------------+-------------+---------------+-----+------------+---------+\n","|          1| 45|     M|             75|          900|             12|   No|         1.0|     45.0|\n","|          2| 30|     F|             60|          720|              6|  Yes|         0.0|     30.0|\n","|          3| 50|     M|             85|         1020|             24|   No|         1.0|     50.0|\n","|          4| 35|     F|             70|          840|             12|  Yes|         1.0|     35.0|\n","|          5| 55|     M|             95|         1140|             24|   No|         1.0|     55.0|\n","|          6| 40|     F|             80|          960|              6|   No|         1.0|     40.0|\n","|          7| 25|     M|             55|          660|              6|  Yes|         0.0|     25.0|\n","|          8| 60|     F|            100|         1200|             12|   No|         1.0|     60.0|\n","|          9| 50|     M|             90|         1080|             24|   No|         1.0|     50.0|\n","|         10| 35|     F|             65|          780|              6|  Yes|         1.0|     35.0|\n","+-----------+---+------+---------------+-------------+---------------+-----+------------+---------+\n","\n"]}],"source":["# CAST notation from SQL\n","# Convert a column to a different data type & read it to df\n","df = df.selectExpr(\"*\",\"cast(age as float) as age_float\")\n","df.show()"]},{"cell_type":"code","execution_count":84,"id":"152c613f","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:21.263769Z","iopub.status.busy":"2024-01-26T05:20:21.26335Z","iopub.status.idle":"2024-01-26T05:20:21.864919Z","shell.execute_reply":"2024-01-26T05:20:21.863033Z"},"papermill":{"duration":0.676679,"end_time":"2024-01-26T05:20:21.868381","exception":false,"start_time":"2024-01-26T05:20:21.191702","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+\n","|age_int|\n","+-------+\n","|     45|\n","|     30|\n","|     50|\n","|     35|\n","|     55|\n","|     40|\n","|     25|\n","|     60|\n","|     50|\n","|     35|\n","+-------+\n","\n"]}],"source":["# If you want to select just the converted column\n","df.selectExpr(\"cast(age_float as integer) as age_int\").show()"]},{"cell_type":"code","execution_count":85,"id":"bbebde50","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:22.022596Z","iopub.status.busy":"2024-01-26T05:20:22.021271Z","iopub.status.idle":"2024-01-26T05:20:22.598029Z","shell.execute_reply":"2024-01-26T05:20:22.596862Z"},"papermill":{"duration":0.656478,"end_time":"2024-01-26T05:20:22.605708","exception":false,"start_time":"2024-01-26T05:20:21.94923","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+---+------+---------------+-------------+---------------+-----+------------+---------+\n","|customer_id|age|gender|monthly_charges|total_charges|contract_length|churn|age_above_30|age_float|\n","+-----------+---+------+---------------+-------------+---------------+-----+------------+---------+\n","|          1| 45|     M|             75|          900|             12|   No|         1.0|     45.0|\n","|          2| 30|     F|             60|          720|              6|  Yes|         0.0|     30.0|\n","|          3| 50|     M|             85|         1020|             24|   No|         1.0|     50.0|\n","|          4| 35|     F|             70|          840|             12|  Yes|         1.0|     35.0|\n","|          5| 55|     M|             95|         1140|             24|   No|         1.0|     55.0|\n","|          6| 40|     F|             80|          960|              6|   No|         1.0|     40.0|\n","|          7| 25|     M|             55|          660|              6|  Yes|         0.0|     25.0|\n","|          8| 60|     F|            100|         1200|             12|   No|         1.0|     60.0|\n","|          9| 50|     M|             90|         1080|             24|   No|         1.0|     50.0|\n","|         10| 35|     F|             65|          780|              6|  Yes|         1.0|     35.0|\n","+-----------+---+------+---------------+-------------+---------------+-----+------------+---------+\n","\n"]}],"source":["# And via SQL tables\n","# Register the DataFrame as a temporary table\n","df.createOrReplaceTempView(\"table\")\n","df.show()"]},{"cell_type":"code","execution_count":86,"id":"46c46034","metadata":{"execution":{"iopub.execute_input":"2024-01-26T05:20:22.756143Z","iopub.status.busy":"2024-01-26T05:20:22.755708Z","iopub.status.idle":"2024-01-26T05:20:23.340915Z","shell.execute_reply":"2024-01-26T05:20:23.33961Z"},"papermill":{"duration":0.661632,"end_time":"2024-01-26T05:20:23.344429","exception":false,"start_time":"2024-01-26T05:20:22.682797","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+\n","|age_float2|\n","+----------+\n","|      45.0|\n","|      30.0|\n","|      50.0|\n","|      35.0|\n","|      55.0|\n","|      40.0|\n","|      25.0|\n","|      60.0|\n","|      50.0|\n","|      35.0|\n","+----------+\n","\n"]}],"source":["# Convert. column to a different data type using SQL syntax\n","query = \"\"\"\n","SELECT\n","    CAST(age AS FLOAT) AS age_float2\n","FROM table\n","\"\"\"\n","\n","spark.sql(query).show()"]},{"cell_type":"markdown","id":"3d948489","metadata":{"papermill":{"duration":0.071187,"end_time":"2024-01-26T05:20:23.494423","exception":false,"start_time":"2024-01-26T05:20:23.423236","status":"completed"},"tags":[]},"source":["**Credit:**\n","https://www.kaggle.com/code/shtrausslearning/mldsai-pyspark-daily\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":2917909,"sourceId":5058245,"sourceType":"datasetVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":221.844068,"end_time":"2024-01-26T05:20:26.187797","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-26T05:16:44.343729","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}