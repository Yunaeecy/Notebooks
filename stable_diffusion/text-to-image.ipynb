{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yunasheng/text-to-image?scriptVersionId=161403134\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"When you think of diffusion models, text-to-image is usually one of the first things that come to mind. Text-to-image generates an image from a text description(for example,\"Astronaut in a jungle, cold color palette, mutes colors, detailed, 8k\") whih is also known as a prompt.\n\nFrom a very high level, a diffusion model takes a prompt and some random initial noise, and iterativelt removes the noise to construct an image. The denoising process is guided by the prompt, and once the denoising process ends after a predetermind number of time steps, the image representaion is decoded into an image.","metadata":{}},{"cell_type":"markdown","source":"We can generate images from a prompt in two steps:","metadata":{}},{"cell_type":"markdown","source":"1. | Load a checkpoint into the AutoPipelineForText2Image class, which automatically detects the appropriate pipeline class to use based on the checkpoint:","metadata":{}},{"cell_type":"code","source":"!pip install diffusers==0.23.1","metadata":{"execution":{"iopub.status.busy":"2024-01-31T05:58:30.290015Z","iopub.execute_input":"2024-01-31T05:58:30.290342Z","iopub.status.idle":"2024-01-31T05:58:44.952845Z","shell.execute_reply.started":"2024-01-31T05:58:30.290312Z","shell.execute_reply":"2024-01-31T05:58:44.951692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-01-31T05:58:44.95698Z","iopub.execute_input":"2024-01-31T05:58:44.957311Z","iopub.status.idle":"2024-01-31T05:59:14.907034Z","shell.execute_reply.started":"2024-01-31T05:58:44.957281Z","shell.execute_reply":"2024-01-31T05:59:14.905978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. |Pass a prompt to the pipeline to generate an image:","metadata":{}},{"cell_type":"code","source":"image = pipeline(\n    \"stained glass of darth vader, blacklight, centered composition, masterpiece, photorealistic, 8k\").images[0]\nimage","metadata":{"execution":{"iopub.status.busy":"2024-01-31T05:59:14.908707Z","iopub.execute_input":"2024-01-31T05:59:14.909537Z","iopub.status.idle":"2024-01-31T05:59:30.819097Z","shell.execute_reply.started":"2024-01-31T05:59:14.909499Z","shell.execute_reply":"2024-01-31T05:59:30.818099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Popular models","metadata":{}},{"cell_type":"markdown","source":"The most common text-to-image models are Stable Diffusion V1.5, Stable Diffusion XL(SDXL), and Kandinsky2.2. There are also ControlNet models or adapters that can be udes with text-to-image models for more direct control in generating images. The results from each model are slightly different because of their architecture and training process, but no matter which model you chose, their usage is more or less the same. Let's use the same prompt for each model and compare their results.","metadata":{}},{"cell_type":"markdown","source":"# Stable Diffusion v1.5","metadata":{}},{"cell_type":"markdown","source":"Stable Diffusion v1.5 is a latent diffusion model initialized from Stable Diffusionv1.4, and finetuned for 595k steps on ` 512*512`  images from the LAION-Aesthetics V2 dataset V2 dataset. You can use this model like this:","metadata":{}},{"cell_type":"code","source":"from diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\ngenerator = torch.Generator(\"cuda\").manual_seed(31)\nimage = pipeline(\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\", generator=generator).images[0]\nimage","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stable Duffusion XL","metadata":{}},{"cell_type":"markdown","source":"SDXL is a much larger version of the previous Stable Diffusion models, and involves a two-stage model process that adds even more details to an image. It also includes some additional *micro-conditionings* to generate high-quality images centered subjects. Take a look at the more comprehensive SDXL guide to learn more about how to use it. In general, you can use SDXL like:","metadata":{}},{"cell_type":"code","source":"from diffusers impoer AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\ngenerator = torch.Generator(\"cuda\").manual_seed(31)\nimage = pipeline(\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\", generator=generator).images[0]\nimage","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kandinsky 2.2","metadata":{}},{"cell_type":"markdown","source":"The Kandisky model is a bit different from the Stable Diffusion models because it also usea an image prior model to create embeddings that are used to better align text and images in the diffusion model.","metadata":{}},{"cell_type":"markdown","source":"The easiest way to use Kandinsky 2.2 is:","metadata":{}},{"cell_type":"code","source":"from diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"kandisky-community/kandisky-2-2-decoder\", torch_dtype=torch.float16).to(\"cuda\")\ngenerator = torch.Generator(\"cuda\").manual_seed(31)\nimage = pipeline(\"Austronaut in a jungle, cold color palette,muted colors, detailed, 8k\", generator=generator).images[0]\nimage","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ControlNet","metadata":{}},{"cell_type":"markdown","source":"ControlNet models are auxiliary models or adaptera that are finetuned on top of text-to-image models, such as Stable **Diffusion v1.5**. Using ControlNet models in combination with text-to-image models offers diverse options for more explicit control over how to generate an image. With ControlNet, you add an additional conditioning input image to the model.For example, if you provide an image of a human pose(usually represented as multiple keypoints that are connected into a skeleton) as a conditioning input, the model generates an image that follows the pose of the image. Check out the more in-depth **ControlNet** guide to learn more about other conditioning inputs and how to use them.","metadata":{}},{"cell_type":"markdown","source":"In this example,let's condition the ControlNet with a human pose estimation image. Load the ControlNet model pretrained on human pose estimations:","metadata":{}},{"cell_type":"code","source":"from diffusers import ControlNetModel, AutoPipelineForText2Image\nfrom diffusers.utils import load_image\nimport torch\n\ncontrolnet = ControlNetModel.from_pretrained(\n    \"lllyasviel/control_v11p_sd15_openpose\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\npose_image = load_image(\"https://huggingface.co/lllyasviel/control_v11p_sd15_openpose/resolve/main/images/control.png\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pass the controlnet to the **AutoPipelineForText2Image** , and provide the promptand pose estimation image:","metadata":{}},{"cell_type":"code","source":"pipeline = AutoPipelineForText1Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\ngenerator = torch.generator(\"cuda\").manual_seed(31)\nimage = pipeline(\"Astronaut in a jungle, cold color palette, muted colors, detained, 8k\", image=pose_image, generator=generator).images[0]\nimage","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configure pipeline parameters","metadata":{}},{"cell_type":"markdown","source":"There are a number of parameters that can be configured in the pipeline that affect how an image is generated. You can change the image's size, specify a negative prompt to improve image quality, and more. This section dives deeper into how to use those parameters.","metadata":{}},{"cell_type":"markdown","source":"# Height ans width","metadata":{}},{"cell_type":"markdown","source":"The height and width parameters control the height and width (in pixels) of the generated image. By defaut, the Stable Diffusion v1.5 model outputs `512*512` images, but youcan change this to any sixe that is a multiple of `8` . For example, to create a rectangular image:","metadata":{}}]}