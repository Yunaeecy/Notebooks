{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yunasheng/image-to-image-2?scriptVersionId=162604763\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install diffusers==0.23.1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-11T08:06:43.105106Z","iopub.execute_input":"2024-02-11T08:06:43.105457Z","iopub.status.idle":"2024-02-11T08:06:57.736545Z","shell.execute_reply.started":"2024-02-11T08:06:43.10543Z","shell.execute_reply":"2024-02-11T08:06:57.735322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stable Diffusion V1.5\nStable Diffusion v1.5 is a latent diffusion model initialized from an earlier checkpoint, and further finetuned for 595K steps on 512x512 images. To use this pipeline for image-to-image, you‚Äôll need to prepare an initial image to pass to the pipeline. Then you can pass a prompt and the image to the pipeline to generate a new image:","metadata":{}},{"cell_type":"code","source":"import torch\n\nprint(torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T08:09:29.309339Z","iopub.execute_input":"2024-02-11T08:09:29.310691Z","iopub.status.idle":"2024-02-11T08:09:29.315683Z","shell.execute_reply.started":"2024-02-11T08:09:29.310657Z","shell.execute_reply":"2024-02-11T08:09:29.314546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import make_image_grid, load_image\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\n\n# prepare image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\ninit_image = load_image(url)\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\n# pass prompt and image to pipeline\nimage = pipeline(prompt, image=init_image).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T08:06:57.738837Z","iopub.execute_input":"2024-02-11T08:06:57.739239Z","iopub.status.idle":"2024-02-11T08:07:40.314435Z","shell.execute_reply.started":"2024-02-11T08:06:57.739201Z","shell.execute_reply":"2024-02-11T08:07:40.313363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stable Diffusion XL (SDXL)\n\nSDXL is a more powerful version of the Stable Diffusion model. It uses a larger base model, and an additional refiner model to increase the quality of the base model‚Äôs output. Read the SDXL guide for a more detailed walkthrough of how to use this model, and other techniques it uses to produce high quality images.","metadata":{}},{"cell_type":"code","source":"# import torch\n# from diffusers import AutoPipelineForImage2Image\n# from diffusers.utils import make_image_grid, load_image\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\n\n# prepare image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-sdxl-init.png\"\ninit_image = load_image(url)\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\n# pass prompt and image to pipeline\nimage = pipeline(prompt, image=init_image, strength=0.5).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kandinsky 2.2\n\nThe Kandinsky model is different from the Stable Diffusion models because it uses an image prior model to create image embeddings. The embeddings help create a better alignment between text and images, allowing the latent diffusion model to generate better images.","metadata":{}},{"cell_type":"code","source":"# import torch\n# from diffusers import AutoPipelineForImage2Image\n# from diffusers.utils import make_image_grid, load_image\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\n\n# prepare image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\ninit_image = load_image(url)\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\n# pass prompt and image to pipeline\nimage = pipeline(prompt, image=init_image).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configure pipeline parameters\n\nThere are several important parameters you can configure in the pipeline that‚Äôll affect the image generation process and image quality. Let‚Äôs take a closer look at what these parameters do and how changing them affects the output.\n\nStrength\n\nstrength is one of the most important parameters to consider and it‚Äôll have a huge impact on your generated image. It determines how much the generated image resembles the initial image. In other words:\n\nüìà a higher strength value gives the model more ‚Äúcreativity‚Äù to generate an image that‚Äôs different from the initial image; a strength value of 1.0 means the initial image is more or less ignored\nüìâ a lower strength value means the generated image is more similar to the initial image\n\nThe strength and num_inference_steps parameters are related because strength determines the number of noise steps to add. For example, if the num_inference_steps is 50 and strength is 0.8, then this means adding 40 (50 * 0.8) steps of noise to the initial image and then denoising for 40 steps to get the newly generated image.","metadata":{}},{"cell_type":"code","source":"# import torch\n# from diffusers import AutoPipelineForImage2Image\n# from diffusers.utils import make_image_grid, load_image\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\n\n# prepare image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\ninit_image = load_image(url)\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\n# pass prompt and image to pipeline\nimage = pipeline(prompt, image=init_image, strength=0.8).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Guidance scale\n\nThe guidance_scale parameter is used to control how closely aligned the generated image and text prompt are. A higher guidance_scale value means your generated image is more aligned with the prompt, while a lower guidance_scale value means your generated image has more space to deviate from the prompt.\n\nYou can combine guidance_scale with strength for even more precise control over how expressive the model is. For example, combine a high strength + guidance_scale for maximum creativity or use a combination of low strength and low guidance_scale to generate an image that resembles the initial image but is not as strictly bound to the prompt.","metadata":{}}]}