{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yunasheng/understanding-word-embeddings-in-pytorch?scriptVersionId=222309093\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"ccd43052","metadata":{"papermill":{"duration":0.003795,"end_time":"2025-02-13T07:22:50.887866","exception":false,"start_time":"2025-02-13T07:22:50.884071","status":"completed"},"tags":[]},"source":["You can't build a large language model without first mastering the concept of embeddings. Fortunately, it's a straightforward idea and a perfect starting point for this series.\n","\n","So, let's say you have a bunch of words. It could be just a simple array of strings.\n"]},{"cell_type":"code","execution_count":1,"id":"0f016500","metadata":{"execution":{"iopub.execute_input":"2025-02-13T07:22:50.895923Z","iopub.status.busy":"2025-02-13T07:22:50.895491Z","iopub.status.idle":"2025-02-13T07:22:50.904208Z","shell.execute_reply":"2025-02-13T07:22:50.903195Z"},"papermill":{"duration":0.014289,"end_time":"2025-02-13T07:22:50.905716","exception":false,"start_time":"2025-02-13T07:22:50.891427","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["{'cat': 0, 'dog': 1, 'rat': 2, 'pig': 3}"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["animals = [\"cat\", \"dog\", \"rat\", \"pig\"]\n","animal_to_idx = {animal: idx for idx, animal in enumerate(animals)}\n","animal_to_idx\n","# Output\n","# {'cat': 0, 'dog': 1, 'rat': 2, 'pig': 3}"]},{"cell_type":"markdown","id":"bd780164","metadata":{"papermill":{"duration":0.00315,"end_time":"2025-02-13T07:22:50.91249","exception":false,"start_time":"2025-02-13T07:22:50.90934","status":"completed"},"tags":[]},"source":["Of course, once you've done all your math, you're going to need to convert the indices back into the words they represent. Here's one way to do that:"]},{"cell_type":"code","execution_count":2,"id":"5a9091a1","metadata":{"execution":{"iopub.execute_input":"2025-02-13T07:22:50.920241Z","iopub.status.busy":"2025-02-13T07:22:50.919895Z","iopub.status.idle":"2025-02-13T07:22:50.925777Z","shell.execute_reply":"2025-02-13T07:22:50.924782Z"},"papermill":{"duration":0.011528,"end_time":"2025-02-13T07:22:50.927327","exception":false,"start_time":"2025-02-13T07:22:50.915799","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["{0: 'cat', 1: 'dog', 2: 'rat', 3: 'pig'}"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["idx_to_animal = {idx: animal for animal, idx in animal_to_idx.items()}\n","idx_to_animal\n","\n","# Output\n","# {0: 'cat', 1: 'dog', 2: 'rat', 3: 'pig'}"]},{"cell_type":"markdown","id":"56a804ed","metadata":{"papermill":{"duration":0.003157,"end_time":"2025-02-13T07:22:50.934182","exception":false,"start_time":"2025-02-13T07:22:50.931025","status":"completed"},"tags":[]},"source":["A better approach is to use one-hot encoding. A one-hot vector is a fancy term for an array where only one element is set to 1 (hot or active), and all other elements are 0. This representation eliminates any unintended ordinal relationships between words."]},{"cell_type":"code","execution_count":3,"id":"6a062f7a","metadata":{"execution":{"iopub.execute_input":"2025-02-13T07:22:50.942218Z","iopub.status.busy":"2025-02-13T07:22:50.941842Z","iopub.status.idle":"2025-02-13T07:22:50.950002Z","shell.execute_reply":"2025-02-13T07:22:50.949195Z"},"papermill":{"duration":0.013817,"end_time":"2025-02-13T07:22:50.951465","exception":false,"start_time":"2025-02-13T07:22:50.937648","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["{'cat': array([1, 0, 0, 0]),\n"," 'dog': array([0, 1, 0, 0]),\n"," 'rat': array([0, 0, 1, 0]),\n"," 'pig': array([0, 0, 0, 1])}"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","n_animals=len(animals)\n","\n","animal_to_onehot={}\n","for idx,animal in enumerate(animals):\n","    one_hot=np.zeros(n_animals, dtype=int)\n","    one_hot[idx]=1\n","    animal_to_onehot[animal]=one_hot\n","\n","animal_to_onehot"]},{"cell_type":"markdown","id":"3a377c63","metadata":{"papermill":{"duration":0.003248,"end_time":"2025-02-13T07:22:50.958355","exception":false,"start_time":"2025-02-13T07:22:50.955107","status":"completed"},"tags":[]},"source":["As you can see, there are no implicit relationships between the words now.\n","\n","One-hot encodings are obviously very sparse representations, and are ideal only if you are dealing with a small number of words. Imagine you had 10,000 words. Then, each encoding would have 9,999 zeroes and a single one. That's ridiculously inefficient. Why waste memory with all those zeroes…\n","\n","Time to create dense vectors for our words. In other words, we're going to create word embeddings now.\n","\n","An embedding representation is a dense vector where most (or all) of the values are non-zero. In machine learning, especially in natural language processing and recommendation systems, dense vectors are used to represent features of words (or sentences, or other entities) in a compact and meaningful way. More importantly, they can capture meaningful relationships between these features.\n","\n","Let's, as an example, create an embedding where the number of features we want is 2, and the number of words we have is 4.\n","\n","Creating an embedding representation with PyTorch is easy. All we need to do is use an nn.Embedding layer. Think of it as a lookup table where the rows represent each unique word, and the columns represent the features of that word (the word's dense vector)."]},{"cell_type":"code","execution_count":4,"id":"2341a5f5","metadata":{"execution":{"iopub.execute_input":"2025-02-13T07:22:50.966561Z","iopub.status.busy":"2025-02-13T07:22:50.966221Z","iopub.status.idle":"2025-02-13T07:22:54.941024Z","shell.execute_reply":"2025-02-13T07:22:54.939859Z"},"papermill":{"duration":3.981328,"end_time":"2025-02-13T07:22:54.943149","exception":false,"start_time":"2025-02-13T07:22:50.961821","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","embedding_layer = nn.Embedding(num_embeddings=4, embedding_dim=2)"]},{"cell_type":"markdown","id":"40550999","metadata":{"papermill":{"duration":0.003409,"end_time":"2025-02-13T07:22:54.950414","exception":false,"start_time":"2025-02-13T07:22:54.947005","status":"completed"},"tags":[]},"source":["Okay, now, let's turn the indices of our words into embeddings. That's almost trivial because all we need to do is pass them to the nn.Embedding layer."]},{"cell_type":"code","execution_count":5,"id":"0d763691","metadata":{"execution":{"iopub.execute_input":"2025-02-13T07:22:54.958942Z","iopub.status.busy":"2025-02-13T07:22:54.958421Z","iopub.status.idle":"2025-02-13T07:22:54.997134Z","shell.execute_reply":"2025-02-13T07:22:54.995995Z"},"papermill":{"duration":0.045073,"end_time":"2025-02-13T07:22:54.99907","exception":false,"start_time":"2025-02-13T07:22:54.953997","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["tensor([0, 1, 2, 3])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["indices=torch.tensor(np.arange(0,len(animals)))\n","indices"]},{"cell_type":"code","execution_count":6,"id":"9d1d8f17","metadata":{"execution":{"iopub.execute_input":"2025-02-13T07:22:55.009064Z","iopub.status.busy":"2025-02-13T07:22:55.008696Z","iopub.status.idle":"2025-02-13T07:22:55.054317Z","shell.execute_reply":"2025-02-13T07:22:55.053349Z"},"papermill":{"duration":0.051889,"end_time":"2025-02-13T07:22:55.056024","exception":false,"start_time":"2025-02-13T07:22:55.004135","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["tensor([[-0.2016, -0.2808],\n","        [ 1.2635, -0.1881],\n","        [-0.1596, -0.3713],\n","        [ 1.2671,  0.5738]], grad_fn=<EmbeddingBackward0>)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["embeddings = embedding_layer(indices)\n","embeddings"]},{"cell_type":"markdown","id":"2c1bcaca","metadata":{"papermill":{"duration":0.003622,"end_time":"2025-02-13T07:22:55.063712","exception":false,"start_time":"2025-02-13T07:22:55.06009","status":"completed"},"tags":[]},"source":["We can now use the indices to see what each word's embedding looks like.\n"]},{"cell_type":"code","execution_count":7,"id":"6ad7fe4c","metadata":{"execution":{"iopub.execute_input":"2025-02-13T07:22:55.072482Z","iopub.status.busy":"2025-02-13T07:22:55.072123Z","iopub.status.idle":"2025-02-13T07:22:55.082952Z","shell.execute_reply":"2025-02-13T07:22:55.08191Z"},"papermill":{"duration":0.017015,"end_time":"2025-02-13T07:22:55.084465","exception":false,"start_time":"2025-02-13T07:22:55.06745","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["cat's embedding is tensor([-0.2016, -0.2808], grad_fn=<SelectBackward0>)\n","dog's embedding is tensor([ 1.2635, -0.1881], grad_fn=<SelectBackward0>)\n","rat's embedding is tensor([-0.1596, -0.3713], grad_fn=<SelectBackward0>)\n","pig's embedding is tensor([1.2671, 0.5738], grad_fn=<SelectBackward0>)\n"]}],"source":["for animal, _ in animal_to_idx.items():\n","  print(f\"{animal}'s embedding is {embeddings[animal_to_idx[animal]]}\")"]},{"cell_type":"markdown","id":"62603fe5","metadata":{"papermill":{"duration":0.003731,"end_time":"2025-02-13T07:22:55.092379","exception":false,"start_time":"2025-02-13T07:22:55.088648","status":"completed"},"tags":[]},"source":["Each word has two features — exactly what we wanted. The numbers don't mean much currently because the embedding layer is not trained. But once it's trained appropriately, the features become meaningful.\n","\n","Note: The features, while crucial for the model, wouldn't necessarily make sense to humans ever. They represent abstract characteristics that are learned through training. To us, these features might appear random or meaningless, but to a trained model, they capture important patterns and relationships that enable it to understand and process the data effectively.\n","\n","We'll learn how to run the training in the next article of this series."]},{"cell_type":"markdown","id":"aad77cd8","metadata":{"papermill":{"duration":0.003521,"end_time":"2025-02-13T07:22:55.0998","exception":false,"start_time":"2025-02-13T07:22:55.096279","status":"completed"},"tags":[]},"source":["# Credit:\n","\n","https://freedium.cfd/https://medium.com/@hathibel/understanding-word-embeddings-in-pytorch-ad9a981d3398"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30886,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":8.602626,"end_time":"2025-02-13T07:22:56.627059","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-02-13T07:22:48.024433","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}